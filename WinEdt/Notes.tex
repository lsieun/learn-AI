\documentclass[UTF8]{ctexart}
\usepackage{amsmath}
\usepackage{color}

   \author{刘森}
   \title{跟我学人工智能}
\begin{document}
    \maketitle
你好，世界 hello, world % This is comment



\section{机器学习算法}

不同的算法，本身没有好坏之分，有的只是，根据不同的场景选择合适的算法。

线性回归和Logistic回归，虽然听起来都叫作“回归”，但其实两者却是做不一样的事情：一个是做连续数据的预测，一个是做离散数据的预测；一个是真正做回归的，一个是做分类的，它们两个【用途】是完全不一样的。【如何推导出来？】线性回归是用高斯分布的方式推导出来，Logistic回归既然是做分类，就用Bnody分布，两点分布来推导出来。两者大的工具都是【最大似然估计】。在线性回归里面，要讨论一个东西：【最小二乘法的本质是什么】。或者说，为什么有最小二乘法呢？有没有最小三乘法呢？有没有最小四乘法呢？在【线 性回归】和【Logistic回归】中强调两个工具：【梯度下降算法】和【极大似然估计】。

\subsection{线性回归}
高斯分布

极大似然估计MLE

最小二乘法的本质

\subsubsection{什么是线性回归}
线性回归
$y=ax+b$\\

考虑多个变量情形，例如两个变量，$h_{\theta}(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}$ ，可以写成如下形式：

\begin{displaymath}
h_{\theta}(x)=\sum_{i=0}^{n}\theta_{i}x_{i}=\theta^{T}x
\end{displaymath}

其中，$\theta$展开后，呈现如下形式：

\[ \left( \begin{array}{c}
\theta_{0} \\
\theta_{1} \\
\theta_{2} \end{array} \right)\]

其中，$x$展开后，呈现如下形式：

%\[ \left( \begin{array}{c}
%\x_{0} \\
%\x_{1} \\
%\x_{2} \end{array} \right)\]
\[ \left( \begin{array}{c}
1 \\
x_{1} \\
x_{2} \end{array} \right)\]

上式中的$1$就表示$x_{0}$，而相应的$\theta_{0}$表示截距，是比较难以直接解释的。



再把上面的式子拿过来，

\begin{displaymath}
h_{\theta}(x)=\sum_{i=0}^{n}\theta_{i}x_{i}=\theta^{T}x
\end{displaymath}

在$h_{\theta}(x)$中，$x$看起来是【自变量】，但事实上是【样本】，所以$x$是已知的，而$\theta$是未知的，我们要通过某一种办法来求解出$\theta$，这个就是【线性回归要解决的问题】。

第05课《回归》00:10:30

目前讲的问题是【what】，即什么是线性回归。过一会儿，会讲【how】，用什么样的工具去求，如何去求的问题。

\subsubsection{使用极大似然估计解释最小二乘}

第05课《回归》00:20:00

使用极大似然估计解释最小二乘

\begin{displaymath}
y^{(i)}=\theta^{T}xY{(i)}+\epsilon^{(i)}
\end{displaymath}

the $\epsilon^{(i)}$ are distributed IID (independently and identically distributed) according to a Gaussian distribution (also called a Normal distribution) with mean $zero$ and some variance $\sigma^{2}$.

误差$\epsilon^{(i)}(1 \leq i \leq m)$是独立同分布的，服从均值为$0$，方差为某定值$\sigma^{2}$的【高斯分布】。原因：【中心极限定理】，可以查阅一下“中心极限定理的意义”。

似然函数 第05课《回归》00:21:21

首先，两边是相等的：

\begin{displaymath}
y^{(i)}=\theta^{T}x^{(i)}+\epsilon^{(i)}
\end{displaymath}

其中，$x^{(i)}$表示第$i$个【样本】，$\theta^{T}x^{(i)}$表示第$i$个样本的【预测值】，$y^{(i)}$表示第$i$个样本的【真实值】，而$\epsilon^{(i)}$表示第$i$个样本的误差。

根据【中心极限定理】，$\epsilon^{(i)}$应该是呈现一个高斯分布的形态。

\begin{displaymath}
p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(\epsilon^{(i)})^{2}}{2\sigma^{2}})
\end{displaymath}

另外，$\epsilon^{(i)} = y^{(i)} - \theta^{T}x^{(i)}$，此时将$\epsilon^{(i)}$代入上式：

\begin{displaymath}
p(y^{i}|x^{i};\theta)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)} - \theta^{T}x^{(i)})^{2}}{2\sigma^{2}})
\end{displaymath}

如此一来，上式当中就没有误差$\epsilon$了，因此只要指定了$x$和$\theta$，就可以认为是一个$y$的分布。换句话讲，$y$其实服从的是【均值是$\theta^{T}x$，方差是某一个$\sigma$的高斯分布（正态分布）】。

那么，用什么可以估计这个$\theta$呢？答：【最大似然估计】。

在上面的公式中，$i$只是表示第$i$个样本，假设一共有$m$个样本，那么，【$m$个样本的似然估计】就可以表示为：

\begin{displaymath}
L(\theta)=\prod_{i=1}^{m}p(y^{i}|x^{i};\theta) = \prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)} - \theta^{T}x^{(i)})^{2}}{2\sigma^{2}})
\end{displaymath}

如此一来，怎么求$\theta$呢？直接对【似然函数】取对数，然后再想办法。

高斯的对数似然与最小二乘


\begin{equation}
\begin{aligned}
l(\theta)&=\log L(\theta)\\
&=\log \prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)} - \theta^{T}x^{(i)})^{2}}{2\sigma^{2}})\\
&=\sum_{i=1}^{m} \log \frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)} - \theta^{T}x^{(i)})^{2}}{2\sigma^{2}})\\
&=m\log \frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{\sigma^{2}} \cdot \frac{1}{2} \sum_{i=1}^{m}(y^{(i)} - \theta^{T}x^{(i)})^{2}
\end{aligned}
\end{equation}

现在，其实是通过【最大似然估计】加上【高斯分布】来得到了【最小二乘法】目标函数。换句话说，这就是解释的“为什么会有最小二乘法”这个概念。

\begin{equation}
\begin{aligned}
J(\theta)=\frac{1}{2} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^{2}
\end{aligned}
\end{equation}

\subsubsection{$\theta$的解析式的求解过程}

$\theta$的解析式的求解过程 第05课《回归》00:29:52

将$M$个$N$维样本组成矩阵$X$：
（1）$X$的每一行对应一个样本，共$M$个样本（measurements）；
（2）$X$的每一列对应样本的一个维度，共$N$维（regressors）
（3）还有额外的一维常数项$x^{(i)}_{0}$，全为$1$。

目标函数：

\begin{equation}
\begin{aligned}
J(\theta)=\frac{1}{2} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^{2}=\frac{1}{2} (X\theta-y)^{T}(X\theta-y)
\end{aligned}
\end{equation}

梯度：

\begin{equation}
\begin{aligned}
\nabla_{\theta}J(\theta)&=\nabla_{\theta}(\frac{1}{2}(X\theta-y)^{T}(X\theta-y))\\
&=\nabla_{\theta}(\frac{1}{2}(\theta^{T}X^{T}-y^{T})(X\theta-y))\\
&=\nabla_{\theta}(\frac{1}{2}(\theta^{T}X^{T}X\theta-\theta^{T}X^{T}y-y^{T}X\theta+y^{T}y))\\
&=\frac{1}{2}(2X^{T}X\theta-X^{T}y-(y^{T}X)^{T})\\
&=X^{T}X\theta-X^{T}y
\end{aligned}
\end{equation}

在上式中，求驻点，令$X^{T}X\theta-X^{T}y=0$。

所以$\theta$取值如下：

\begin{equation}
\begin{aligned}
\theta=(X^{T}X)^{-1}\cdot X^{T}y
\end{aligned}
\end{equation}

\subsubsection{最小二 乘法意义下的参数最优解}

参数的解析解

\begin{equation}
\begin{aligned}
\theta=(X^{T}X)^{-1} X^{T}y
\end{aligned}
\end{equation}

若$X^{T}X$不可逆或防止过拟合，增加$\lambda$扰动

\begin{equation}
\begin{aligned}
\theta=(X^{T}X+\lambda I)^{-1} X^{T}y
\end{aligned}
\end{equation}

第05课《回归》00:38:36

“简单”方法记忆结论

\begin{equation}
\begin{aligned}
&X\theta=y\\
&\Rightarrow X^{T}X\theta=X^{T}y\\
&\Rightarrow \theta = (X^{T}X)^{-1} X^{T}y
\end{aligned}
\end{equation}

\subsubsection{梯度下降算法Gradient Descent}

第05课《回归》01:14:49

事实上，通过解析解的方式$\theta = (X^{T}X)^{-1} X^{T}y$来求解$\theta$是没有问题的，真的是对的；但是，我们往往在机器学习中，习惯在这个地方引出“梯度下降算法”，并且也习惯使用“梯度下降算法”来求解$\theta$。我们这里还是求目标函数$J(\theta)=\frac{1}{2} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^{2}$的梯度，延着负梯度方向不停的下降，降到某一个值，降不下去了，我们就可以知道，得到了一个局部的极小值，这个局部的极小值点，或许就是我们想要的$\theta$。

初始化$\theta$（随机初始化）

沿着负梯度方向迭代，更新后的$\theta$使$J(\theta)$更小

\begin{equation}
\begin{aligned}
\theta_{j}=\theta_{j}-\alpha \cdot \frac{\partial J(\theta)}{\partial \theta}
\end{aligned}
\end{equation}

其中，$\alpha$表示学习率、步长。这个步长$\alpha$事实上是有办法可以指定比较优的，后续再讲怎么选$\alpha$会更优。

\subsubsection{梯度方向}
梯度方向 第05课《回归》01:15:37

\begin{equation}
\begin{aligned}
\frac{\partial}{\partial \theta_{j}} J(\theta)&=\frac{\partial}{\partial \theta_{j}} \frac{1}{2} (h_{\theta}(x)-y)^{2}\\
&=2 \cdot \frac{1}{2} (h_{\theta}(x)-y) \cdot \frac{\partial}{\partial \theta_{j}}(h_{\theta}(x)-y)\\
&=(h_{\theta}(x)-y) \cdot \frac{\partial}{\partial \theta_{j}}(\sum_{i=0}^{n}\theta_{i}x_{i}-y)\\
&=(h_{\theta}(x)-y)x_{j}
\end{aligned}
\end{equation}

这里是求偏导，一共有$n$个特征，当前求解的是第$j$个特征的偏导。得到这个梯度之后，我们就可以延着负梯度的方向下降下去就可以了。

这里有个需要注意的地方，我们求出来的是梯度，而我们要用的是\textbf{负梯度}，而\textbf{负梯度}应该是$-(h_{\theta}(x)-y)x_{j}$，因此可以调换一下$h_{\theta}(x)$和$y$的位置，写成这种形式$(y-h_{\theta}(x))x_{j}$。

\subsubsection{批量梯度下降算法}

Repeat until convergence(会聚; 集收敛)

\begin{equation}
\begin{aligned}
\theta_{j} := \theta_{j}+\alpha \sum_{i=1}^{m} (y^{(i)}-h_{\theta}(x^{(i)}))x^{(i)}_{j}
\end{aligned}
\end{equation}

\textbf{gradient descent}. Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient always converges (assuming the learning rate $\alpha$ is not too large) to the global minimum. Indeed, $J$ is a convex(凸形; 凸的) quadratic(二次的;) function.

 \subsubsection{随机梯度下降算法}

 for $i=1$ to $m$

 \begin{equation}
\begin{aligned}
\theta_{j} := \theta_{j}+\alpha  (y^{(i)}-h_{\theta}(x^{(i)}))x^{(i)}_{j}
\end{aligned}
\end{equation}

This algorithm is called \textbf{stochastic gradient descent} (also \textbf{incremental gradient descent}).  Whereas batch gradient descent has to scan through the entire training set before taking a single step -- a costly operation if $m$ is large -- stochastic gradient descent can start making progress right away, and continues to make progress with each example it looks at. Often, stochastic gradient descent gets $\theta$ "close" to the minimum much faster than batch gradient descent. (Note however that it may never "converge" to the minimum, and the parameters $\theta$ will keep oscillating(振荡; （使） 摆动) around the minimum of $J(\theta)$; but in practice most of the values near the minimum will be reasonably good approximations to the true minimum.) For these reasons, particularly when the training set is large, stochastic gradient descent is often preferred over batch gradient descent.

在没有明确\textbf{批量梯度下降}和\textbf{随机梯度下降}哪个更优的情况下，优先选择“随机梯度下降”。

在求解$\theta$的时候，不一定非要是最好的，它只能说是\textbf{堪用}的，能够work的，可用的就行了。有的时候，不要追求完美，完美往往是达不到的，只要能够过得去， 还可以，就行了。

\subsubsection{折中：mini-batch}

第05课《回归》01:22:37

如果不是每拿到一个样本即更改梯度，而是若干个样本的平均梯度作为更新方向，则是mini-batch梯度下降算法。

机器学习(Machine learning)，第一，要会理论，它是一个能够走多远的基础；第二，是要会写代码，它是保证当前的工作能够持续下去。“渔”和“鱼”都得要。

机器学习中，很多时间都花在了\textbf{如何建模型、如何调参、如何选特征、如何优化模型}这些事情上，写代码并没有那么的困难。

\subsubsection{权值的设置}

高斯核函数 第05课《回归》01:45:26

我们希望用“线性回归”求得模型的“残差”服从高斯分布（正态分布）；如果不服从高斯分布，我们就去重新选“特征”。如果不服从高斯分布，说明有些“特征”没有被考虑进来。


\subsection{逻辑回归：分类问题的首选算法}

第05课《回归》02:00:30

\subsubsection{Logistic/sigmoid函数}


 \begin{equation}
\begin{aligned}
g(z)=\frac{1}{1+e^{-z}}
\end{aligned}
\end{equation}

 \begin{equation}
\begin{aligned}
h_{\theta}(x)=g(\theta^{T}x)=\frac{1}{1+e^{-\theta^{T}x}}
\end{aligned}
\end{equation}

 \begin{equation}
\begin{aligned}
g'(x)&=(\frac{1}{1+e^{-x}})'=\frac{e^{-x}}{(1+e^{-x})^{2}}\\
&=\frac{1}{1+e^{-x}} \cdot \frac{e^{-x}}{1+e^{-x}}\\
&=\frac{1}{1+e^{-x}} \cdot (1- \frac{1}{1+e^{-x}})\\
&=g(x) \cdot (1-g(x))
\end{aligned}
\end{equation}

\subsubsection{Logistic回归参数估计}

假定

 \begin{equation}
\begin{aligned}
P(y=1|x;\theta)&=h_{\theta}(x)\\
P(y=0|x;\theta)&=1-h_{\theta}(x)\\
\end{aligned}
\end{equation}

上面的两个式子，可以用一个式子来表示：

 \begin{equation}
\begin{aligned}
p(y|x;\theta)=(h_{\theta}(x))^{y}(1-h_{\theta}(x))^{1-y}
\end{aligned}
\end{equation}

那么似然函数$L(\theta)$这样求：

\begin{equation}
\begin{aligned}
L(\theta)&=p(\vec{y}|X;\theta)\\
&=\prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\theta)\\
&=\prod_{i=1}^{m}(h_{\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}
\end{aligned}
\end{equation}

\subsubsection{对数似然函数}

第05课《回归》02:01:23

\begin{equation}
\begin{aligned}
l(\theta) &= \log L(\theta) \\
&= \sum_{i=1}^{m} y^{(i)}\log h_{\theta}(x^{(i)}) + (1-y^{(i)}) \log (1-h_{\theta}(x^{(i)}))
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\frac{\partial}{\partial \theta_{j}} l(\theta) &= (y \frac{1}{g(\theta^{T} x)} - (1-y) \frac{1}{1-g(\theta^{T} x)}) \frac{\partial}{\partial \theta_{j}} g(\theta^{T} x)\\
&=  (y \frac{1}{g(\theta^{T} x)} - (1-y) \frac{1}{1-g(\theta^{T} x)}) g(\theta^{T} x)(1-g(\theta^{T} x))\frac{\partial}{\partial \theta_{j}} \theta^{T}x \\
&= (y(1-g(\theta^{T} x))-(1-y)g(\theta^{T} x))x_{j}\\
&= (y-h_{\theta}(x))x_{j}
\end{aligned}
\end{equation}

\subsubsection{参数的迭代}

第05课《回归》02:01:55

Logistic回归参数的学习规则

\begin{equation}
\begin{aligned}
\theta_{j} := \theta_{j} + \alpha (y^{(i)}-h_{\theta}(x^{(i)}))x^{(i)}_{j}
\end{aligned}
\end{equation}

比较上面的结果和“线性回归”的结论的差别：它们具有相同的形式，Logistic回归是用于做分类的，属于广义线性回归。

\subsubsection{对数线性模型}

一个事件的\textbf{几率}odds，是指该\textbf{事件发生的概率}与\textbf{事件不发生的概率}的比值。

对数几率：$\log it$函数

 \begin{equation}
\begin{aligned}
P(y=1|x;\theta)&=h_{\theta}(x)\\
P(y=0|x;\theta)&=1-h_{\theta}(x)\\
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\log it(p) &= \log \frac{p}{1-p}\\
&= \log \frac{h_{\theta}(x)}{1-h_{\theta}(x)}\\
&=\log \frac{\frac{1}{1+e^{-\theta^{T}x}}}{\frac{e^{-\theta^{T}x}}{1+e^{-\theta^{T}x}}}\\
&= \theta^{T}x
\end{aligned}
\end{equation}

说一个事儿：如果有一个$x_{1}$和$x_{2}$，正常去求$x_{1}$占多大比例的方法是

\begin{equation}
\begin{aligned}
\frac{x_{1}}{x_{1}+x_{2}}
\end{aligned}
\end{equation}

如果不想这么算，而是想做一个指数的比例，那么$x_{1}$和$x_{2}$就变成了$e^{x_{1}}$和$e^{x_{2}}$，再求比例就是

\begin{equation}
\begin{aligned}
\frac{e^{x_{1}}}{e^{x_{1}}+e^{x_{2}}} = \frac{1}{1+e^{x_{2}-x_{1}}}
\end{aligned}
\end{equation}

如果令$x_{2}-x_{1}=-z$，就得到：

\begin{equation}
\begin{aligned}
\frac{1}{1+e^{-z}}
\end{aligned}
\end{equation}

这样就得到了sigmoid函数。

\subsection{工具}
梯度下降算法\\
极大似然估计\\

\subsection{Softmax}

\subsection{聚类}

按道理来说，“算法”和“模型”是两个完全不同的概念，算法一般用于表示解决特定的数学问题，而模型则侧重于解决实际的问题，但在不同场景下，两者也会经常混用。

“聚类”是无监督的机器学习算法，而“线性回归、Logistic回归、Softmax、SVM、随机森林”都是有监督的机器学习算法。

“聚类”并不像“有监督分类”那样有$y$值，而只有$m \times n$维的向量数据，其中$m$表示$m$个样本，$n$表示有$n$个特征。“聚类”并不依赖于$y$值，而是依赖于$m \times n$维的数据，根据其内部之间的相似性，来做聚类。

一个$m \times n$维的数据，经过某种聚类算法之后，这$m$个样本聚类到$k$个簇当中，这样就把一个$m \times n$维矩阵转换成了一个$m \times k$维矩阵，这本质上就是一个聚类的过程。我们发现，这个数据是从一个$m \times n$维矩阵转换成了一个$m \times k$维矩阵，这本质上又是一个降维的过程。所以“聚类”这个词和“降维”这个词，两者本质上是一样的。

\subsubsection{本次目标}

掌握\textbf{K-means聚类}的思路和使用条件

了解\textbf{层次聚类}的思路和方法

理解\textbf{密度聚类}并能够应用于实践：（1）DBSCAN；（2）密度最大值聚类

掌握\textbf{谱聚类}的算法：考虑谱聚类和\textbf{PCA}的关系。

\subsubsection{聚类的定义}

聚类就是对大量未知标注的数据集，按数据的\textcolor{red}{内在相似性}将数据集划分为\textcolor{red}{多个类别}，使\textcolor{red}{类别内的数据相似度较大}而\textcolor{blue}{类别间的数据相似度较小}。

而这种\textbf{内在相似性}，通常用\textbf{相似度}或\textbf{距离}来度量。往往，\textbf{距离}求出来之后，将距离的$-1$次方作为\textbf{相似度}。

\subsubsection{相似度/距离计算方法总结}

第10课 聚类 00:11:18

闵可夫斯基距离Minkowski

\begin{equation}
\begin{aligned}
dist(X,Y)=\left ( \sum_{i=1}^{n} |x_{i}-y_{i}|^{p} \right )^{\frac{1}{p}}
\end{aligned}
\end{equation}

在上式中，当$p=2$时，就是标准的“欧氏距离”；当$p=1$时，就是“曼哈顿距离”；当$p=\infty$时，就相当于取$|x_{i}-y_{i}|$的最大距离，就称为“切比雪夫距离”。

杰卡德相似系数（Jaccard），从集合的角度来理解

\begin{equation}
\begin{aligned}
J(A,B)=\frac{|A \bigcap B|}{|A \bigcup B|}
\end{aligned}
\end{equation}

余弦相似度（cosine similarity），从角度来理解

\begin{equation}
\begin{aligned}
\cos (\theta)=\frac{a^{T}b}{|a| \cdot |b|}
\end{aligned}
\end{equation}

Pearson相关系数（Pearson CorrelationCoefficient）是用来衡量两个数据集合是否在一条线上面，它用来衡量定距变量间的线性关系。

\begin{equation}
\begin{aligned}
\rho_{XY} &= \frac{cov(X,Y)}{\sigma_{X} \sigma_{Y}} \\
&= \frac{E[(X-\mu_{X})(Y-\mu_{Y})]}{\sigma_{X} \sigma_{Y}}\\
&= \frac{\sum_{i=1}^{n}(X_{i}-\mu_{X})(Y_{i}-\mu_{Y})}{\sqrt{\sum_{i=1}^{n}(X_{i}-\mu_{X})^{2}}\sqrt{(\sum_{i=1}^{n}Y_{i}-\mu_{Y})^{2}}}
\end{aligned}
\end{equation}

假设$A=(A_{1},A_{2},...A_{n})$，有$n$个数，我们可以求它的平均值$\bar{A}$，那么$\frac{1}{n}(A_{i}-\bar{A})^{2}$就可以认为是随机变量A的均方差。同时，有$B=(B_{1},B_{2},...B_{n})$，也有$n$个数，它的均方差也可以表示为$\frac{1} {n}(B_{i}-\bar{B})^{2}$。也可以用$\frac{1} {n}(A_{i}-\bar{A})(B_{i}-\bar{B})$来度量$A$和$B$之间的协方差。这个$A$和$B$之间协方差，也可以除以它的标准差，就有了Pearson相关系数。

相对熵（K-L距离）

\begin{equation}
\begin{aligned}
D(p \parallel q) = \sum_{x}^{ }p(x) \log \frac{p(x)}{q(x)}=E_{p(x)}\log \frac{p(x)}{q(x)}
\end{aligned}
\end{equation}

Hellinger距离

\begin{equation}
\begin{aligned}
D_{\alpha}(p \parallel q) = \frac{2}{1-a^{2}} \left ( 1- \int p(x)^{\frac{1+\alpha}{2}} q(x)^{\frac{1-\alpha}{2}} dx\right )
\end{aligned}
\end{equation}

注意：不要拘泥于某种相似性的计算方式，在实践当中，有些场景，适合用欧氏距离，如果是推荐系统，就习惯用杰卡德系数，如果是文本的相似度，就可能用余弦相似度，这都是有可能的。

我们要说明的就是，在“聚类”算法当中，可以任意挑选一个“相似度计算方法”，就能够算出样本$i$和样本$j$的相似性$S_{ij}$；如果有$m$个样本，就形成一个$m \times m$的相似度方阵，后面就对$m \times m$的方阵使用各种手段来去做聚类。至于说用哪一个“相似度”算法，则是一个相对独立的事情。

\subsubsection{Hellinger distance}

第10课 聚类 00:20:50

\subsubsection{余弦相似度与Pearson相似系数}

第10课 聚类 00:24:50

$n$维向量$x$和$y$的夹角记作$\theta$，根据余弦定理，其余弦值为：

\begin{equation}
\begin{aligned}
\cos (\theta) = \frac{x^{T}y}{|x| \cdot |y|}=\frac{\sum_{i=1}^{n}x_{i}y_{i}}{\sqrt{\sum_{i=1}^{n}x^{2}_{i}}\sqrt{\sum_{i=1}^{n}y^{2}_{i}}}
\end{aligned}
\end{equation}

这两个向量的Pearson相关系数是：

\begin{equation}
\begin{aligned}
\rho_{XY} &= \frac{cov(X,Y)}{\sigma_{X} \sigma_{Y}} \\
&= \frac{E[(X-\mu_{X})(Y-\mu_{Y})]}{\sigma_{X} \sigma_{Y}}\\
&= \frac{\sum_{i=1}^{n}(X_{i}-\mu_{X})(Y_{i}-\mu_{Y})}{\sqrt{\sum_{i=1}^{n}(X_{i}-\mu_{X})^{2}}\sqrt{(\sum_{i=1}^{n}Y_{i}-\mu_{Y})^{2}}}
\end{aligned}
\end{equation}

如果此时的$\mu_{X}$和$\mu_{Y}$都为0，则此时的Pearson相关系数恰好是“余弦相似度”。

相关系数即将$x$、$y$坐标向量各自\textcolor{red}{平移到原点后的夹角余弦}！

这即解释了为何文档间求距离使用\textcolor{blue}{夹角余弦}——因为这一物理量表征了文档\textcolor{green}{去均值化}后随机向量间\textcolor{blue}{相关系数}。

\subsubsection{聚类的基本思想}

第10课 聚类 00:27:32

给定一个有$N$个对象的数据集，构造数据的$k$个簇，$k \leq n$。满足下列条件：

（1）每一个簇至少包含一个对象

（2）每一个对象属于且仅属于一个簇

（3）将满足上述条件的$k$个簇称作一个合理划分。

基本思想：对于给定的类别数目$k$，首先给出初始划分，通过迭代改变\textbf{样本}和\textbf{簇}的隶属关系，使得每一次改进之后的划分方案都\textcolor{red}{较前一次好}。

\subsubsection{K-means算法}

K-means算法，也称为k-平均或k-均值，是一种广泛使用的聚类算法，或者成为其他聚类算法的基础。

假定输入样本为$S=x_{1},x_{2},...x_{m}$，则算法步骤为：

（1）选择初始的$k$个类别中心$\mu_{1},\mu_{2},...\mu_{k}$

（2）对于每个样本$x_{i}$，将其标记为距离类别中心最近的类别，即：

\begin{equation}
\begin{aligned}
label_{i} = \underset{1 \leq j \leq k}{\arg \min}  \parallel x_{i}-\mu_{j} \parallel
\end{aligned}
\end{equation}

（3）将每个类别中心更新为隶属该类别的所有样本的均值

\begin{equation}
\begin{aligned}
\mu_{j} = \frac{1}{|c_{j}|} \sum_{i \in c_{j}}^{ }x_{j}
\end{aligned}
\end{equation}

（4）重复最后两步，直到类别中心的变化小于某阈值。

中止条件：迭代次数、簇中心变化率、最小平方误差MSE(Minimum Squared Error)

很好的一个问题就是：（1）K-means算法中的k如何选择？（2）k个中心如何初始化？

对于第一个问题，一般是使用两种方式。第一种，有的时候，可以通过“先验的知识”来确定的，比如说，抛骰子只能有6种可能的数值。第二种，就是交叉验证，不断的尝试k的大小，来看看最小平方误差是否会减小。当k没有更好的办法选择时，只能够通过相互交叉验证的方式帮助我们做。

第二个问题解答 第10课 聚类 00:39:00

对于第二个问题，一种是随机的给定初始值。

\subsubsection{K-means的公式化解释}

第10课 聚类 00:55:32

记$K$个簇中心为$\mu_{1},\mu_{2},...\mu_{k}$，每个簇的样本数目为$N_{1},N_{2},...,N_{k}$

使用平方误差作为目标函数：

\begin{equation}
\begin{aligned}
J(\mu_{1},\mu_{2},...\mu_{k}) = \frac{1}{2} \sum_{j=1}^{K} \sum_{i=1}^{N_{j}} (x_{i}-\mu_{j})^{2}
\end{aligned}
\end{equation}

该函数为关于$\mu_{1},\mu_{2},...\mu_{k}$的凸函数，其驻点为：

\begin{equation}
\begin{aligned}
\frac{\partial J}{\partial \mu_{j}} = \sum_{x_{i} \in \{J\} }^{ }(x_{i}-\mu_{j}) = 0 \\
\Rightarrow \mu_{j}=\frac{1}{N_{j}} \sum_{x_{i} \in \{J\}}^{ }x_{i}
\end{aligned}
\end{equation}

k-均值的聚类结果，一定是类“圆”的。

\subsubsection{K-means聚类方法总结}

第10课 聚类 01:01:32

优点：

（1）是解决聚类问题的一种经典算法，简单，快速

（2）对处理大数据集，该算法保持可伸缩性和高效率

（3）当簇近似为高斯分布时，它的效果较好

缺点：

（1）在簇的平均值可被定义的情况下才能使用，可能不适用于某些应用

（2）必须事先给出$k$（要生成的簇的数目），而且对初值敏感，对于不同的初始值，可能会导致不同结果

（3）不适合于发现非凸形状的簇或者大小差别很大的簇

（4）对噪声和孤立点数据敏感

可作为其他聚类方法的基础算法，如谱聚类。

\subsubsection{对K-means的思考：K-Mediods聚类(K中值距离)}

第10课 聚类 01:02:40

K-Means将簇中所有点的均值作为新质心，若簇中含有异常点，将导致均值偏离严重。以一维数据为例：

（1）数据1、2、3、4、100的均值为22，显然距离“大多数”数据1、2、3、4比较远

（2）改成求数组的中位数3，在该实例中更为稳妥。

（3）这种聚类方式即\textcolor{red}{K-Mediods聚类}（K中值距离）

初值的选择，对聚类结果有影响吗？如何避免？

\subsubsection{轮廓系数(Silhouette)}

第10课 聚类 01:07:37

Silhouette系数是对聚类结果有效性的解释和验证，由Peter J. Rousseeuw于1986年提出。

计算样本$i$到同簇其他样本的平均距离$a_{i}$。$a_{i}$越小，说明样本$i$越应该被聚类到该簇。将$a_{i}$称为样本$i$的\textcolor{blue}{簇内不相似度}。簇$C$中所有样本的$a_{i}$均值称为簇$C$的\textcolor{red}{簇不相似度}。

计算样本$i$到其他某簇$C_{j}$的所有样本的平均距离$b_{ij}$，称为样本$i$与簇$C_{j}$的不相似度。定义为样本$i$的\textcolor{blue}{簇间不相似度}：$b_{i}=\min \{ b_{i1},b_{i2},...b_{iK} \}$。其中，$b_{i}$越大，说明样本$i$越不属于其他簇。

根据样本$i$的簇内不相似度$a_{i}$和簇间不相似度$b_{i}$，定义样本$i$的轮廓系数：

\begin{displaymath}
s(i)=\frac{b(i)-a(i)}{\max \{ a(i),b(i)\}}
\end{displaymath}

$s_{i}$接近于1，则说明样本$i$聚类合理；$s_{i}$接近于-1，则说明样本$i$更应该聚类到另外的簇；若$s_{i}$近似为0，则说明样本$i$在两个簇的边界上。

所有的样本的$s_{i}$的均值，称为聚类结果的轮廓系数，是该聚类是否合理、有效的度量。

\subsubsection{层次聚类方法}

第10课 聚类 01:19:19

层次聚类方法对给定的数据集进行层次的分解，直到某种条件满足为止。具体又分为：（1）凝聚的层次聚类：AGNES算法，（2）分裂的层次聚类：DIANA算法。

凝聚的层次聚类（AGNES算法），是一种自底向上的策略，首先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇，直到某个终结条件被满足。

分裂的层次聚类（DIANA算法），采用自顶向下的策略，它首先将所有对象置于一个簇中，然后逐渐细分为越来越小的簇，直到达到了某个终结条件。

\subsubsection{密度聚类方法}

第10课 聚类 01:22:39

密度聚类算法的指导思想是，只要样本点的密度大于某阈值，则将该样本添加到最近的簇中。

这类算法能克服基于距离的算法只能发现“类圆形”的聚类的缺点，可发现任意形状的聚类，且对噪声数据不敏感。但计算密度单元的计算复杂度大，需要建立空间索引来降低计算量。

常用的两个算法：DBSCAN算法 和 密度最大值算法。

DBSCAN算法（Density-Based Spatial Clustering of Applications with Noise）是一个比较有代表性的基于密度的聚类算法。与划分和层次聚类方法不同，它将簇定义为\textcolor{red}{密度相连的点的最大集合}，能够把具有足够高密度的区域划分为簇，并可在有“噪声”的数据中发现任意形状的聚类。

密度最大值聚类是一种简洁优美的聚类算法，可以识别各种形状的类簇，并且参数很容易确定。

第10课 聚类 01:40:39

\subsubsection{谱和谱聚类}

第10课 聚类 01:57:06

方阵作为线性算子，它的所有特征值的全体统称方阵的\textcolor{red}{谱}。

（1） 方阵的谱半径为最大的特征值
（2）矩阵$A$的\textcolor{red}{谱半径}：$A^{T}A$的最大特征值

谱聚类是一种基于图论的聚类方法，通过对样本数据的\textcolor{red}{拉谱拉斯矩阵}的\textcolor{red}{特征向量}进行聚类，从而达到对样本数据聚类的目的。

\section{提升}

bagging 并行 RF

boosting 串行 GDBT Adaboost

GBDT分类案例   CART（Classification And Regression Tree） 回归树


Adaboost Adaptive Boosting 自适应增加

问题：（1）选择哪个特征合适；（2）选取该特征的某个值进行切分呢？





\section{支持向量机SVM}

\subsection{复习：对偶问题}


第09课 SVM 00:01:22 复习：对偶问题

一般优化问题的Lagrange乘子法

Lagrange函数

【lsieun】“仿射”和“线性变换”似乎是同一个意思。

第09课 SVM 00:01:22 复习：Lagrange对偶函数(dual function)

概念：KKT条件

【lsieun】Lagrange函数的最大值，就等于“原函数”的最小值。这里主要是讲Lagrange乘子法作为一种工具，将原来求“最小值”的问题，转换为求“最大值”的问题。

第09课 SVM 00:03:42 线性方程的最小二乘问题

此处是举例子，用于解释上面的“由最小值转换成求最大值的解决方法”。

第09课 SVM 00:04:26 强对偶条件

若要对偶函数的最大值即为原问题的最小值，考察需要满足的条件

第09课 SVM 00:05:37 强对偶KKT条件: Karush-Kuhn-Tucker

\subsection{主要内容和目标}

理解\textcolor{red}{支持向量机SVM}的原理和目标  \textcolor{blue}{SVM核心的东西 what}.

掌握支持向量机的\textcolor{red}{计算过程}和算法步骤  \textcolor{blue}{SVM核心的东西 how}.

理解\textcolor{red}{软间隔最大化}的含义：（1）对线性不可分的数据给出（略有错误）的分割面；（2）线性可分的数据需要使用“软间隔”目标函数吗？

了解\textcolor{red}{核函数}的思想

了解\textcolor{red}{SMO}算法的过程

核函数：SVM本身是个线性分类器，那么在原始的SVM上，可以加上一些核函数，来构造一个非线性的分隔面，来更好的解决分类问题。

\subsection{各种概念}

第09课 SVM 00:09:32 各种概念

SVM进行简单的化分，可以分为三类：

第一类，\textbf{线性可分}支持向量机：（1）硬间隔最大化 hard margin maximization；（2）硬间隔支持向量机

第二类，\textbf{线性}支持向量机：（1）软间隔最大化 soft margin maximization；（2）软间隔支持向量机

第三类，\textbf{非线性}支持向量机：（1）核函数 kernel function

从学习的角度来说，第一类（线性可分支持向量机）是最重要的，只要学会了第一类，稍微加一点东西就能变成第二类（线性支持向量机），对第二类稍微加一点东西就能变成第三类（非线性支持向量机）。[知识梯度lsieun]

在实际应用中，使用的较多的就是第二类和第三类了。

\subsection{分隔超平面}

第09课 SVM 00:13:37 分隔超平面

[lsieun]什么是超平面？概念怎么理解，有时间查一查。之前查的时候，印象中是说，超过二维的平面，都叫超平面。

第09课 SVM 00:13:37 分隔超平面的思考

如何定义两个集合的“最优”分割超平面？（1）找到集合“边界”上的若干点，以这些点为“基础”计算超平面的方向；以两个集合边界上的这些点的平均作为超平面的“截距”；（2）支持向量：support vector

若两个集合有部分相交，如何定义超平面，使得两个集合“尽量”分开？

第09课 SVM 00:16:00 线性分类问题

假定一共有$N$个样本，最终可能只有$n$个样本参与到支持向量(support vector)里去了，一般而言，$N$是大于$n$的，甚至是$N$远大于$n$的，就比如说有10000个样本，有20个样本参与到支持向量里去了，相当于有9980都是零，只有20个非零，所以SVM是个稀疏的模型。在有些教材中，会特意将SVM放在“稀疏模型”中介绍。

第09课 SVM 00:20:00 CNN 卷积神经网络也是个稀疏模型。在这一点上（都是稀疏模型），SVM和CNN是相似的。如果在面试中谈到了，或许是一个加分项呢，哈哈。。。

事实上，有很多很多条直线可以将两部分图形分开，但是谁是最优的直线呢？如果我们知道了“哪条直线是最优的”，我们又怎么把“目标函数”写出来呢？我们只有写出了“目标函数”，才能下一步去“如何优化它”。所以，首先要有目标函数。此处解决的是“有和无”（目标函数）的问题，之后才是“优化”的问题。

\subsection{SVM的目标函数}

第09课 SVM 00:24:00 画图讲解“空间求距离”

在做SVM的时候，不要过多考虑究竟是多少维的问题，讲课的时候，画在纸上，用两维的数据是为了方便，其实它的算法/计算过程是一样的。推导的时候，是用二维来做，但真正做的时候，与N维是没有区别的。

sign函数：符号函数（一般用sign(x)表示）是很有用的一类函数，能够帮助我们在几何画板中实现一些直接实现有困难的构造。 符号函数 能够把函数的符号析离出来 。在数学和计算机运算中，其功能是取某个数的符号（正或负）： 当$x>0$，$sign(x)=1$;当$x=0$，$sign(x)=0$; 当$x<0$， $sign(x)=-1$。

第09课 SVM 00:32:00 很精彩的部分 SVM的目标函数：求最小值的最大值，哈哈

\subsection{求解SVM的目标函数}

\subsubsection{输入数据}

第09课 SVM 00:39:00 输入数据

假设给定一个特征空间上的训练数据集$T=\{(x_{1},y_{1}),(x_{2},y_{2})...(x_{N},y_{N})\}$，其中，$x_{i} \in R^{n}$，$y_{i} \in \{+1,-1\}$，$i=1,2,...N$。

$x_{i}$为第$i$个实例（若$n>1$，则$x_{i}$为向量）。

$y_{i}$为$x_{i}$的类标记：（1）当$y_{i}=+1$时，称$x_{i}$为正例；（2）（1）当$y_{i}=-1$时，称$x_{i}$为负例。

$(x_{i},y_{i})$称为样本点。

[一个非常好的问题]为什么$y_{i}$的取值是+1和-1呢？\\ 第09课 SVM 00:44:00
在做logistic 回归的时候，y一个可以取1，一个可以取0。到了SVM里面呢，y一个取+1，一个取-1。为什么会这样呢？所有教科书中都不会讲为什么。事实上，logistic回归，也可以用+1和-1去推导，是可以做的。在SVM里面用+1和-1是为了方便，仅此而已，方便推导。方便在哪儿了呢？如果y等于+1或-1，那么$y_{i} \cdot f(x) = \frac{f(x)}{y_{i}}$，这样的话，就方便我们做推导。

[问题]SVM中实际当中用的多吗？\\ 第09课 SVM 00:45:00

[问题]SVM和Spark? SVM的分类效果，真的是好，一般而言，SVM的分类结果优于logistic回归、优于随机森林（RF），但是SVM的计算速度慢。比如，几千个，几万个样本，SVM不做优化的情况下，用到分钟级（时间）能够把参数学出来，但是随机森林，秒级（时间）就能搞定。SVM往往是一个比较好的分类器。

[问题]SVM如何做多分类呢？

\subsubsection{线性可分支持向量机}

给定“线性可分训练集”，通过\textcolor{red}{间隔最大化}得到的分隔超平面为$y(x)=w^{T} \Phi (x) + b$，相应的分类决策函数$f(x)=sign(w^{T} \Phi (x) + b)$，该决策函数称为“线性可分支持向量机”。

其中，$\Phi (x)$是某个确定的特征空间转换函数，它的作用是将$x$映射到（更高的）维度。最简单直接的：$\Phi (x)=x$。

稍后会看到，求解分离超平面问题可以等价为求解相应的\textcolor{red}{凸二次规划问题}。

\subsubsection{整理符号}

分割平面：$y(x)=w^{T} \Phi (x) + b$

训练集：$x_{1},x_{2}, \dots ,x_{n}$

目标值：$y_{1},y_{2}, \dots ,y_{n}$

新数据的分类：$sign(y(x))$

[问题] 第09课 SVM 00:48:00 分隔超平面，哪边是+1，哪边是-1，跟w方向有关系吗？是的。如果位于分隔超平面的法向量的正向，就是+1；位于法向量的负向，就是-1。

\subsubsection{推导目标函数}

第09课 SVM 00:49:00 推导目标函数

\subsubsection{最大间隔分离超平面}

第09课 SVM 00:52:00 最大间隔分离超平面

不要忘记了，虽然目标函数是我们的优化目标，但其实是想通过目标函数求解出其中w和b。

\subsubsection{函数间隔和几何间隔}

第09课 SVM 00:54:00 函数间隔和几何间隔

\subsubsection{建立新目标函数}

第09课 SVM 00:58:00 建立新目标函数

\subsection{拉格朗日乘子法}

第09课 SVM 01:15:00 拉格朗日乘子法

目标函数

约束条件

原问题是极小极大问题

原始问题的对偶问题 ，是极大极小问题。

第09课 SVM 01:30:00 整理目标函数：添加负号

\subsection{线性可分支持向量机学习算法}

第09课 SVM 01:33:00 线性可分支持向量机学习算法

构造并求解约束最优化问题

\begin{equation}
\begin{aligned}
\underset{a}{\min} \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} \left ( \Phi(x_{i}) \cdot  \Phi(x_{y}) \right ) - \sum_{i=1}^{n} a_{i} \\
s.t. \sum_{i=1}^{n} a_{i}y_{i}=0 \\
a \geq 0, i=1,2,\dots , n
\end{aligned}
\end{equation}

求得最优解$a^{*}$，再将$a^{*}$代入到下面的方程中去

\begin{equation}
\begin{aligned}
w^{*}=\sum_{i=1}^{N}a_{i}y_{i}\Phi (x_{i}) \\
b^{*}=y_{i}-\sum_{i=1}^{N}a_{i}^{*} \left ( \Phi(x_{i}) \cdot \Phi(x_{j}) \right )
\end{aligned}
\end{equation}

计算求得分离超平面

\begin{equation}
\begin{aligned}
w^{*}\Phi(x)+b^{*}=0
\end{aligned}
\end{equation}

分类决策函数

\begin{equation}
\begin{aligned}
f(x)=sign \left ( w^{*}\Phi(x)+b^{*} \right )
\end{aligned}
\end{equation}

第09课 SVM 01:36:00 将约束带入函数，化简计算

第09课 SVM 01:37:00 现在要说一个重要的结论，只有$\alpha$不为0的向量，才是支撑向量。

\subsection{线性支持向量机}

第09课 SVM 01:39:00 线性支持向量机

不一定分类完全正确的超平面就是最好的。

样本数据本身线性不可分

若数据线性不可分，则增加松弛因子$\varepsilon_{i} \geq 0$，使函数间隔加上松弛变量大于等于1.这样约束条件变成

\begin{equation}
\begin{aligned}
y_{i}(w \cdot x_{i}+b) \geq 1 - \varepsilon_{i}
\end{aligned}
\end{equation}

目标函数：

\begin{equation}
\begin{aligned}
\underset{w,b}{\min} \frac{1}{2} ||w||^{2} + C\sum_{i=1}^{N} \varepsilon_{i}
\end{aligned}
\end{equation}

虽然可以这么做，但是松弛因子$\varepsilon_{i}$不能太过分（不能太大），因此将$\varepsilon_{i}$加起来求和并放到了目标函数当中（$\sum_{i=1}^{N} \varepsilon_{i}$），当作约束条件之一，让整体的值是最小的，这样就得到了线性的SVM。

现在需要来解释一下“目标函数”。首先说$C$，$C$是用来调节松弛因子$\varepsilon_{i}$和原始的函数$\frac{1}{2} ||w||^{2}$之间的比例关系；如果$C$是无穷大的时候，那就意味着：哪怕$\varepsilon_{i}$是一个很小的数，只要乘以$C$，就会使得最后的结果很大，此时就必须强制性的要求$\varepsilon_{i}$为0，如果不为0，就不可能取最小值了。因此，当$C$是无穷大的时候，$\varepsilon_{i}$为0，就相当于退化成了线性可分的SVM。“线性SVM”是“线性可分SVM”的一个推广、泛化。

\subsection{线性SVM的目标函数}

第09课 SVM 01:48:00 线性SVM的目标函数

\subsection{核函数}

第09课 SVM 02:07:00 核函数

可以使用核函数，将原始输入空间映射到新的特征空间，从而使原本线性不可分的样本可能在核空间可分。

多项式核函数$\kappa(x_{1},x_{2})=(||x_{1}-x_{2}||^{a}+r)^{b}$，其中$a,b,r$为常数。

高斯核函数RBF $\kappa(x_{1},x_{2})=\exp \left ( -\frac{||x_{1}-x_{2}||^{2}}{2 \sigma^{2}}\right )$

字符串核函数，例如两字符串的满足某条件的子串的余弦相似度

在实际应用中，往往依赖先验领域知识/交叉验证等方案才能选择有效的核函数。如果没有更多先验信息，则使用高斯核函数。

\subsection{SVM中系数的求解：SMO}

第09课 SVM 02:07:00 SVM中系数的求解：SMO

序列最小最优化 Sequential Minimal Optimization

有多个拉格朗日乘子

每次只选择其中两个乘子做优化，其他因子认为是参数。将N个问题，转换成两个变量的求解问题，并且目标函数是凸的。

\subsection{总结与思考}

第09课 SVM 02:26:00 总结与思考

SVM可以用来划分多分类别吗？（1）直播多分类；（2） 1 vs rest / 1 vs 1

SVM和Logistic回归的比较：（1）经典的SVM，直接输出类别，不给出后验概率；（2）Logistic回归，会给出属于那哪个类别的后验概率。重点：两者目标函数的异同。

SVM框架下引入Logistic函数：输出条件后验概率

SVM用于回归问题：SVR

体会SVM的目标函数的建立过程：原始目标函数和Lagrange函数有什么关系。

\section{贝叶斯网络}

之前的线性回归、逻辑回归，都是已知$X$来求$y$，是建立$X$和$y$的一种直接关系。

SVM中也是已知$X$来求$y$，只不过多了一个$\kappa(x_{1},x_{2})$核函数，也是建立$X$和$y$的一种直接关系。

有一种情况是$X$和$y$之前并不是直接相关的，它们中间有一些我们看不到的其他东西，那这些东西我们就需要建立一个图模型来解释它，我们往往会把$X$和$y$看成某一个随机变量，那么$X$可能服从某个概率密度，$y$可能服从某个概率密度，如此一来，中间的每一个节点就都是概率上的东西，用概率的节点形成的这样一个图，就是概率图模型。如果模型稍微复杂一点，往往用概率图模型来解决问题是需要的，所以内容是非常大的。

\subsection{主要内容}

第14课 贝叶斯网络 00:02:30 主要内容

复习本次将用到知识：相对熵、互信息（信息增益）

朴素贝叶斯

贝叶斯网络的表达：（1）条件概率表示参数个数分析；（2）马尔科夫模型

D-separation：（1）条件相互独立的三种类型；（2）Markov Blanket

网络的构建流程：（1）混合（离散+连续）网络：线性高斯模型；（2）Chow-Liu算法：最大权生成树MSWT。

PLSA---CDA---DL

\subsection{复习}

\subsubsection{复习：相对熵}

第14课 贝叶斯网络 00:02:50 相对熵

相对熵，又称互信息，交叉熵，鉴别信息，Kullback熵，Kullback-Leible散度

设$p(x)$、$q(x)$是$X$中取值的两个概率分布，则$p$对$q$的相对熵是

\begin{equation}
\begin{aligned}
D(p||q)=\sum_{x}p(x)\log \frac{p(x)}{q(x)}=E_{p(x)}\log \frac{p(x)}{q(x)}
\end{aligned}
\end{equation}

说明：（1）相对熵可以度量两个随机变量的“距离”；（2）一般的$D(p||q) \neq D(q||p)$，只有当$p$和$q$相同的时候，等号才成立；（3）$D(p||q) \geq 0$、$D(q||p) \geq 0$

\subsubsection{复习：互信息}

第14课 贝叶斯网络 00:03:45 互信息

两个随机变量$X,Y$的互信息，定义为$X$，$Y$的“联合分布”和“独立分布乘积”的相对熵。

\begin{equation}
\begin{aligned}
I(X,Y)=D\left (P(X,Y) || P(X)P(Y) \right) \\
I(X,Y)=\sum_{x,y}p(x,y)\log \frac{p(x,y)}{p(x)p(y)}
\end{aligned}
\end{equation}

\subsubsection{复习：信息增益}

第14课 贝叶斯网络 00:03:56 信息增益

信息增益，表示得知特征$A$的信息而使类$X$的信息的不确定性减少的程度。

定义：特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$[lsieun:经验熵，我理解为“不知道的东西的信息量”]与特征$A$给定条件下$D$的经验熵$H(D|A)$之差，即$g(D,A)=H(D)-H(D|A)$；显然，这即为训练集$D$和特征$A$的互信息，所以，“信息增益”和“互信息”本质上是一个东西。

\subsection{概率}

第14课 贝叶斯网络 00:04:32 概率

条件概率：$P(A|B)=\frac{P(AB)}{P(B)}$

全概率公式：$P(A)=\sum_{i}P(A|B_{i})P(B_{i})$

贝叶斯（Bayes）公式：$P(B_{i}|A)\frac{P(A|B_{i})P(B_{i})}{\sum_{j}P(A|B_{j})P(B_{j})}$

由“条件概率”和“全概率公式”可以推导出“贝叶斯公式”。

我感觉，自己好像懂了一点儿，但还是不那么特别懂，有时间百度一下。目前的理解是，“贝叶斯公式”是由“结果”求“原因”的概率，由“后”向“前”计算的概率，是“后验”概率。 查一下“先验概率”和“后验概率”到底是什么意思。

第14课 贝叶斯网络 00:04:55 贝叶斯公式带的思考

\subsection{朴素贝叶斯的假设}

第14课 贝叶斯网络 00:13:33 朴素贝叶斯的假设

（1）特征独立性（概率）：一个特征出现的机率，与其他特征（条件）独立。其实是：对于给定分类的条件下，特征独立。

（2）特征均衡性（重要性）：每个特征同等重要。

\subsubsection{以文本分类为例}

第14课 贝叶斯网络 00:14:26 以文本分类为例

样本：10000封邮件，每个邮件被标记为垃圾邮件或非垃圾邮件

分类目标：给定第10001封邮件，确定它是垃圾邮件还是非垃圾邮件

方法：朴素贝叶斯

\subsubsection{拉谱拉斯平滑}

第14课 贝叶斯网络 00:34:41 拉谱拉斯平滑

\subsection{贝叶斯网络}

第14课 贝叶斯网络 00:45:08 贝叶斯网络

\subsubsection{一个简单的贝叶斯网络}

第14课 贝叶斯网络 00:50:39 一个简单的贝叶斯网络

\subsubsection{全连接贝叶斯网络}

第14课 贝叶斯网络 00:52:14 全连接贝叶斯网络

\subsubsection{一个“正常”的贝叶斯网络}

第14课 贝叶斯网络 00:55:12 一个“正常”的贝叶斯网络

\subsubsection{贝叶斯网络的形式化定义}

第14课 贝叶斯网络 01:13:33 贝叶斯网络的形式化定义

\section{主题模型}

\subsection{主要内容}

第15课 主题模型 00:00:58 主要内容

共轭先验分布

多项式分布-Dirichlet分布：二项式分布-Beta分布

LDA模型：Gibbs采样算法

多项分布，比如掷色子，有6个点，如果做N次就是6项分布，它的共轭分布就是Dirichlet分布。如果变成2点投硬币，投N次就是二项分布，二项分布的共轭分布叫Beta分布。

“共轭”在数学、物理、化学、地理等学科中都有出现。 本意：两头牛背上的架子称为轭，轭使两头牛同步行走。共轭即为按一定的规律相配的一对。通俗点说就是孪生。在数学中有共轭复数、共轭根式、共轭双曲线、共轭矩阵等。

“二项分布”就是重复n次独立的伯努利试验。在每次试验中只有两种可能的结果，而且两种结果发生与否互相对立，并且相互独立，与其它各次试验结果无关，事件发生与否的概率在每一次独立试验中都保持不变，则这一系列试验总称为n重伯努利实验，当试验次数为1时，二项分布服从0-1分布。

“多项式分布”（Multinomial Distribution）是二项式分布的推广。
二项分布的典型例子是扔硬币，硬币正面朝上概率为p, 重复扔n次硬币，k次为正面的概率即为一个二项分布概率。把二项分布公式推广至多种状态，就得到了多项分布。

\subsection{引：$\Gamma$函数}

第15课 主题模型 00:01:50 $\Gamma$函数

$\Gamma$函数（读作Gamma函数，拼音“ga ma”函数）是阶乘在实数上的推广。是欧拉发现的。


\subsection{Beta分布}

第15课 主题模型 00:05:36 Beta分布

Beta分布的概率密度

其中系数$B$为

Gamma函数可以看成阶乘的实数域推广

\subsubsection{Beta分布的期望}

第15课 主题模型 00:08:26 Beta分布的期望

\subsection{朴素贝叶斯的分析}

第15课 主题模型 00:17:07 朴素贝叶斯的分析

可以胜任许多文本分类的问题

无法解决语料中“一词多义”和“多词一义”的问题，它更像是词法分析，而非语义分析

如果使用“词向量”作为文档的特征，“一词多义”和“多词一义”会造成计算文档间相似度的不准确性。

可以通过增加“主题”的方式，一定程度的解决上述问题：（1）一个词可能被映射到多个主题中，“一词多义”；（2）多个词可能被映射到某个主题的概率很高，“多词一义”。

\subsection{文档和主题}

第15课 主题模型 00:20:44 文档和主题

是一个标准的无监督学习

LDA是一个主题模型，也是无监督的东西。给了若干个文档，假设给定$k$个主题，其实就是讲若干个文档映射到了$k$个主题上，第1个文档跟第1个主题有多少相似度（相关度），第2个文档跟第1个主题有多少相似度（相关度）……每一个文档跟每一个主题都有一个相似度（相关度），就可以认为这些文档做了$k$个主题“概率化的聚类”，因此，LDA可以从一定程度上是一个“聚类”。这一段主要是讲“LDA”和“聚类（降维）”的关系。

LDA是一个无监督模型，因为只给定了文本$X$，没有告诉我们这些文档属于什么样的主题，只给了$X$，让我们去学那个$y$。我们经常讲，“无监督模型”，往往可以约等于“聚类”，往往可以约等于“降维”，本质上它们往往可以通用。如果你发现一个模型是“无监督的”，你就往里去套，往往你会发现，它就是“聚类”，就是“降维”。

D---Z---W：在文档（D）和词（W）之间加了一个隐变量（Z），有的时候说“主题”或者“话题”是一个意思。这个模型是02、03年才提出的。

这里边涉及到将“文档”进行“切词”的一个过程。注意，“切词”这个概念。

\subsection{LDA涉及的主要问题}

第15课 主题模型 00:29:10 LDA涉及的主要问题

共轭先验分布

Dirichlet分布

LDA模型：Gibbs采样算法学习参数

\subsubsection{共轭先验分布}

第15课 主题模型 00:29:22 共轭先验分布

\subsubsection{复习：二项分布的最大似然估计}

第15课 主题模型 00:38:41 二项分布的最大似然估计

\subsubsection{共轭先验的直接推广}

第15课 主题模型 00:51:22 共轭先验的直接推广

从2到K：（1）二项式   到  多项式；（2）Beta分布 到 Dirichlet分布

\subsubsection{Dirichlet分布}

第15课 主题模型 00:51:54 Dirichlet分布

Beta分布

 Dirichlet分布

\subsubsection{对称Dirichlet分布的参数分析}

第15课 主题模型 01:06:54 对称Dirichlet分布的参数分析

我们从来没有说过“词频”满足Dirichlet分布，大家一定要清楚这个概念，这个“词频”就是一个非常普通的V点分布，“主题”也是一个标准的V项分布，而多项分布是需要有“参数”的，这个参数满足Dirichlet分布。“词”也好，“文档”也好，“主题”也好，它们满足的是正常多项式分布，这样是不是清楚一点。

\subsubsection{Dirichlet分布分析}

第15课 主题模型 01:08:17 Dirichlet分布分析

\subsubsection{参数$\alpha$对Dirichlet分布的影响}

第15课 主题模型 01:16:17 参数$\alpha$对Dirichlet分布的影响

主题的数目$k$是需要事先给定的，但是不需要指定“主题”是什么。主题模型里，相当于要指定什么，$(\alpha , k)$指定$\alpha$等于几，指定$k$等于几，其他的就没有了，其他的参数就不需要用户指定子。

\subsubsection{对称Dirichlet分布}

第15课 主题模型 01:20:17 对称Dirichlet分布

\subsection{LDA的解释}

第15课 主题模型 01:31:17 LDA的解释

共有m篇文章，一共涉及了K个主题；

每篇“文章”（长度为$N_{m}$，第1篇文章长度为$N_{1}$，第2篇文章长度为$N_{2}$，以此类推）都有各自的“主题分布”，“主题分布”是“多项分布”，该“多项分布”的参数服从“Dirichlet分布”，该“Dirichlet分布”的参数为$\alpha$。

每个“主题”都有各自的“词分布”，“词分布”为“多项分布”，该“多项分布”的参数服从“Dirichlet分布”，该“Dirichlet分布”的参数为$\beta$。

对于某篇“文章”中的第$n$个“词”，首先从该“文章”的“主题分布”中采样一个“主题”，然后在这个“主题”对应的“词分布”中采样一个“词”。不断重复这个随机生成过程，直到$m$篇“文章”全部完成上述过程。

\subsection{Gibbs updating rule}

第15课 主题模型 01:50:17 Gibbs updating rule

\subsection{代码实现}

第15课 主题模型 02:06:17 代码实现

\section{卷积神经网络}

00:46:23

神经网络

隐藏层 激活函数

input nodes / hidden nodes / output nodes

一般建议：隐藏层内的节点多一些，而不是隐藏层数多，谷歌的那个狗才5层而已。

relu

\section{Tensorflow}

\subsection{安装Tensorflow}

        conda install tensorflow

        pip install tensorflow

\section{微积分与概率论基础}

第01课 数学分析与概率论 00:00:00

能够在如何用“机器学习”算法的基础之上，再能够知道它为什么是起作用的。所以我们会给大家探讨算法的背后是什么，在“机器学习”的角度来看“数学到底是如何的”。

\subsection{主要内容}

本课程示例概述

机器学习的角度看数学：（1）复习数学分析（常数$e$、导数/梯度、Taylor展开式、凸函数）；（2）概率论基础（古典概型、贝叶斯公式、常见概率分布）

\subsection{什么是机器学习}

第01课 数学分析与概率论 00:01:21 什么是机器学习

 Tom Michael Mitchell在1997年给出了“机器学习”的定义：对于某给定的\textcolor{red}{任务T}(task)，在合理的\textcolor{red}{性能度量方案P}的前提下，某计算机程序可以自主学习\textcolor{red}{任务T}的\textcolor{red}{经验E}；随着提供合适、优质、大量的\textcolor{red}{经验E}，该程序对于\textcolor{red}{任务T}的\textcolor{red}{性能}逐步提高。

这里最重要是机器学习的对象：（1）任务Task,T，一个或多个；（2）经验Experience, E；（3）性能Performance,P

“机器学习”的定义，这样一个描述看起来很严格，但有时候理解起来却不知所云，不知道它在说什么。我们可以简单的理解为：随着“任务”的不断的执行，“经验”的累积会带来计算机“性能”的提升。

\subsubsection{换个表述}

第01课 数学分析与概率论 00:02:59 换个表述

机器学习(Machine Learning)，是“人工智能(AI)”的一个分支。我们使用计算机设计一个\textcolor{red}{系统}，使它能够根据提供的\textcolor{red}{训练数据/样本}按照一定的方式来\textcolor{red}{学习}；随着训练次数的增加，该系统可以在\textcolor{red}{性能}上不断学习和改进；通过\textcolor{red}{参数优化}的学习模型，能够用于\textcolor{red}{预测}相关问题的输出。

现在我们来举一个例子，“机器学习”到底与我们传统的“算法”最大的区别在什么地方。我们举一个经常见的例子“无人驾驶汽车”，思考：如何设计无人驾驶机动车？

汽车的无人汽车模块已经成熟：全自动公共交通工具已经出现在了世界上的多个城市，Lutz探路者/CYCAB/Google。问题是，如何设计自动驾驶系统呢？把所有的交通规则录入到系统中去，人多的时候，人少的时候，应该以多大的速度来行驶，到路口应该怎么做，到了这种情况应该怎么做，到了那种情况应该怎么做，我们把所有的情况都要处理好，然后这个系统就做完了，这是一种思路。这种思路是传统的做法，却不是“机器学习”的算法。这种思路会带来的一个问题，我们不太方便去穷举所有的情况，那我们应该怎么考虑呢？我们就先去做一个简单的、刚刚能满足要求的、能够在最简单的路况上进行自动行驶的系统，遇到某种情况（样本）之后，做参数调整，遇到新的情况（样本），再做参数调整，不停的做这样的事情，经过大量的样本迭代之后，就输出了我们想要的那个“模型参数”。这样一个过程，是通过我们的样本(sample)，我们采样，不停的让系统去学，这就是机器学习的基本想法，所以“机器学习”是一个非常拟人的说法，让机器来学习，不停的学，直到把“参数”学到手。这就是跟传统算法非常非常不一样的东西。“机器学习”是一个很“务实”的东西，它拿到“样本”，拿到“模型”之后，不断的训练“参数”。通过这样一个例子，就比只有一个“严格的定义”要更容易让大家理解到更多的东西。

因此，在机器学习里，我们需要有“样本”，需要建立我们的“模型”，然后需要做我们的“参数”，在后续的学习中，我们需要不停的做这个东西。

我们现在已经了解了“机器学习”的思路了，那么应该如何去做机器学习呢？如何去介入这个领域呢？遇到这种情况，我们会先想“人是怎么学习的呢”。

\subsection{人类的学习？}

如何从完全“无知”到掌握知识？ 语言、颜色、形状等特征统计

有监督学习：月亮

无监督学习：阅兵

增强学习：机器人走路、机器人踢球

\subsection{很精彩}

第01课 数学分析与概率论 00:18:32 很精彩

sample feature target

train test

training/labels/feature vectors/machine learning algorithm/model/expected label

sample分为train和test两部分，第一部分train是用“标记好”的数据做训练，第二部分test是用已经训练好的模型做预测。这是两个阶段，很显然的事情是：我们应该把重点放在前一部分，如何把“模型参数”给学出来，用什么样的算法、什么样的工具、什么样的优化手段，把这个参数学到手，然后至于怎么测试它、怎么用它，相对而言是简单的，将未知的$X$带进去就可以了。

sample数据的格式，可以有text docs、images、sounds和transactions（交易数据）。

[问题]深度学习是什么概念呢？ 第01课 数学分析与概率论 00:19:57

\subsection{机器学习方法}

第01课 数学分析与概率论 00:22:04 机器学习方法

同一批“样本数据”，不同的“模型”做出来的结果是不一样的。因此，在机器学习中，第一步是确定用什么模型来为我们的数据做服务，不同的数据需要有不同的模型来做。这里面有一个认知问题：并不是复杂的模型，就一定是好的，一般同来而言，如果有两个模型可以胜任我们的工作，谁（“模型”）简单用谁做。真不是越复杂越好，而是越简单越好，一般我们把这种思路叫做“奥卡姆剃刀(Occam's Razor)”。

奥卡姆剃刀定律（Occam's Razor, Ockham's Razor）又称“奥康的剃刀”，它是由14世纪逻辑学家、圣方济各会修士奥卡姆的威廉（William of Occam，约1285年至1349年）提出。这个原理称为“如无必要，勿增实体”，即“简单有效原理”。正如他在《箴言书注》2卷15题说“切勿浪费较多东西去做，用较少的东西，同样可以做好的事情。”

奥卡姆剃刀原则是奥卡姆（全称是「奥卡姆的威廉」，「William of Ockham」）当年说过的某句话的前半部分。那一句话是：\textbf{Do not multiply entities beyond necessity}, but also do not reduce them beyond necessity.按照我的理解，这句话的前半句才是剃刀原则。

作者：知乎用户
链接：https://www.zhihu.com/question/20159241/answer/14167100
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

「问题」监督学习和非监督学习都有哪些？ 第01课 数学分析与概率论 00:25:04

监督学习：线性回归、K近邻、逻辑回归、Linear SVM、还有核函数的SVM、决策树、Naive Bayes、

非监督学习：聚类、EM算法、（推荐系统里的）协同过滤、关联分析

聚类是一个最重要的非监督学习方法，甚至所有非监督学习的算法都可以归并到“聚类”的概念里面去。

是否有$y$，是否有“标记/标签”是区分“监督学习”和“非监督学习”的重要标志。

「问题」机器学习和数据挖掘的区别？ 第01课 数学分析与概率论 00:28:23


\subsection{思考：机器如何发现新词}

第01课 数学分析与概率论 00:30:21 思考：机器如何发现新词

PPT当中描述了一种方法，但没有细讲；说是后面会讲到用HMM（隐马尔可夫模型）发现新词。

\subsection{Python Code示例}

第01课 数学分析与概率论 00:32:10 Python Code示例

第01课 数学分析与概率论 00:36:03 线性回归、rate、Loss

EM code

GMM（高斯混合模型）与图像

SVM：高斯核函数的影响        1995-2006年非要重要的算法

贝叶斯网络

理解HMM框架

HMM分词

其他内容：最大熵模型（自然语言处理解决标记问题）、聚类（K-means、K-Mediods、密度聚类、谱聚类）、降维（PCA/SVD/ICA）、SVM（与核技术相结合）、主题模型PLSA/LDA（与聚类、标签传递算法相结合）、条件随机场（无向图模型、链式条件随机场解决标记问题）、变分推导Variation Interface（与EM、贝叶斯相结合、参数、隐变量的学习）、深度学习（大规模人工神经网络）

\subsection{本课程参考书、文献}

第01课 数学分析与概率论 00:38:18 本课程参考文献

\subsection{回忆知识}

第01课 数学分析与概率论 00:40:18 回忆知识

求$S$的值：

\begin{equation}
\begin{aligned}
S=\frac{1}{0!}+\frac{1}{1!}+\frac{1}{2!}+\frac{1}{3!}+\frac{1}{4!}+\dots+\frac{1}{n!}+\dots
\end{aligned}
\end{equation}

这个$S$会有极限吗？如果有，它的极限会是多少呢？

这个极限是存在的，极限等于$e$，为什么呢？

\subsection{复习微积分：两边夹定理}

第01课 数学分析与概率论 00:40:44 复习微积分：两边夹定理

当$x \in U(x_{0},r)$时（$x$是在$x_{0}$的邻域是有定义的），有$g(x) \leq f(x) \leq h(x)$成立，并且$\underset{x \rightarrow x_{0}}{\lim}g(x)=A$，$\underset{x \rightarrow x_{0}}{\lim}h(x)=A$，那么：

\begin{equation}
\begin{aligned}
\underset{x \rightarrow x_{0}}{\lim}f(x)=A
\end{aligned}
\end{equation}

\subsection{极限}

第01课 数学分析与概率论 00:41:40 极限

\subsection{复习微积分：极限存在定理}

第01课 数学分析与概率论 00:48:33 复习微积分：极限存在定理

\subsection{导数}

第01课 数学分析与概率论 00:59:46 导数

简单的说，导数就是曲线的斜率，是曲线变化快慢的反映。

\textcolor{red}{二阶导数}是斜率变化快慢的反映，表征曲线\textcolor{red}{凹凸性}：（1）二阶导数连续的曲线，往往称之为“光滑”的；（2）还记得高中物理老师时常念叨的吗，\textcolor{red}{加速度}的方向总是指向轨迹曲线凹的一侧。

根据$\underset{x \rightarrow \infty}{\lim} \left (  1+\frac{1}{x} \right )^{x}=e$可以得到函数$f(x)=\ln x$的导数，进一步根据换底公式、反函数求导等，得到其他初等函数的导数。

$f(x)=\log _{a}x$，当$a=e$的时候，在$x=1$处点的导数为$1$；当$a=e$的时候，将$f(x)$写成$f(x)=\ln x$。

\subsubsection{常用函数的导数}

第01课 数学分析与概率论 01:01:46 常用函数的导数

$C'=0$

$(x^{n})'=nx^{n-1}$

$(\sin x)'=\cos x$

$(\cos x)'=-\sin x$

$(a^{x})'=a^{x}\ln a$

$(e^{x})'=e^{x}$

$(\log_{a}{x})'=\frac{1}{x}\log_{a}{e}$

$(\ln x)'=\frac{1}{x}$

$(\mu+\nu)'=\mu'+\nu'$

$(\mu\nu)'=\mu'\nu+\mu\nu'$

\subsubsection{应用1}

第01课 数学分析与概率论 01:02:17 应用1

已知函数$f(x)=x^{x}$，其中$x>0$，求$f(x)$的最小值。（1）领会\textcolor{red}{幂指函数}的一般处理思路；（2）在信息熵章节中将再次遇到它。



在算这种“任何函数”取“极值”的时候，往往是对它（该函数）进行“求导”；然后让“导数”等于0。导数为0的点，我们称为“驻点”；然后我们再通过别的方式来判断这个“驻点”是极大值，还是极小值；然后，就能从统一的概念上来看待求极值问题，就是这么一个过程。

\textcolor{red}{幂指函数}取“导数”是不方便的。在“常用函数的导数”中给出了“幂函数”的导数公式和“指数函数”的导数公式，但是却没有给出“幂指函数”的导数公式。这时候，我们应该怎么办呢？

“幂函数”是基本初等函数之一。一般地，形如$y=x^{a}$($a$为有理数）的函数，即以“底数”为自变量，“幂”为因变量，“指数”为常数的函数称为\textbf{幂函数}。例如函数$y=x^{0}$ 、$y=x^{1}$ 、$y=x^{2}$ 、$y=x^{-1}$ 等都是幂函数。

“指数函数”是重要的基本初等函数之一。一般地，$y=a^{x}$函数($a$为常数且以$a>0$，$a \neq 1$)叫做\textbf{指数函数}，函数的定义域是 $R$。

\textbf{幂指函数}既像“幂函数”，又像“指数函数”，二者的特点兼而有之。作为“幂函数”，其“幂指数”确定不变，而“幂底数”为自变量；相反地，“指数函数”却是“底数”确定不变，而“指数”为自变量。\textbf{幂指函数}就是“幂底数”和“幂指数”同时都为“自变量”的函数。这种函数的推广，就是广义幂指函数。

附：$N^{\frac{1}{\log {N}}}=?$ （1）在计算机算法跳跃表Skip List的分析中，用到了该常数。（2）背景：跳表是支持增删改查的动态数据结构，能够达到与平衡二叉树、红黑树近似的效率，而代码实现简单。

有时间百度一下“平衡二叉树”和“红黑树”吧。

\subsubsection{求解$x^{x}$}

第01课 数学分析与概率论 01:04:19 求解$x^{x}$

令$t=x^{x}$，然后取“对数”可以得到$\ln t = x \ln x$；两边对$x$求导，得到$\frac{1}{t}t'=\ln x + 1$；令$t'=0$，得到$\ln x + 1 = 0$；再求得$x=e^{-1}$；再求得$t=e^{-\frac{1}{e}}$。

\subsubsection{积分应用2}

第01课 数学分析与概率论 01:06:21 积分应用2

当$N \rightarrow \infty$的时候（当$N$趋向于无穷大的时候），$\ln N!$大致等于$N(\ln{N}-1)$，即$\ln{N!} \rightarrow  N(\ln{N}-1)$。

细节，我在这里先省略了，后续再看。

\begin{equation}
\begin{aligned}
\ln{N!}&=\sum_{i=1}^{n} \ln{i} \\
&\approx \int_{1}^{N} \ln {xdx} \\
&=x\ln x \mid_{1}^{N}-\int_{1}^{N} xd(\ln x)\\
&=N\ln N - \int_{1}^{N} x\cdot \frac{1}{x}dx \\
&=N\ln N - x\mid_{1}^{N} \\
&=N\ln N - N + 1 \\
&\rightarrow N\ln N - N
\end{aligned}
\end{equation}

\subsubsection{Taylor公式-Maclaurin公式}

第01课 数学分析与概率论 01:10:46 Taylor公式-Maclaurin公式

Taylor公式

$f(x)=f(x_{0})+f'(x_{0})(x-x_{0})+\frac{f''(x_{0})}{2!}(x-x_{0})^{2}+\dots+\frac{f^{(n)}(x_{0})}{n!}(x-x_{0})^{n}+R_{n}(x)$

Maclaurin公式

$f(x)=f(0)+f'(0)x+\frac{f''(0)}{2!}x^{2}+\dots+\frac{f^{(n)}(0)}{n!}x^{n}+o(x^{n})$

Taylor公式和Maclaurin公式的区别在于：Taylor公式是在“任意点”展开，而Maclaurin公式是在$0$点展开，应该说Taylor公式是Maclaurin公式的范化。

我感觉，经常用的就是“Maclaurin公式”，用它来求解一些函数的函数值。下面进行举例。

\subsubsection{Taylor公式的应用1}

第01课 数学分析与概率论 01:11:40 Taylor公式的应用1

数值计算：初等函数值的计算（在原点展开）

\begin{equation}
\begin{aligned}
\sin x = x - \frac{x^{3}}{3!} + \frac{x^{5}}{5!}  - \frac{x^{7}}{7!} + \frac{x^{9}}{9!} + \dots
+ (-1)^{m-1} \frac{x^{2m-1}}{(2m-1)!} + R_{2m}
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
e^{x} = 1 + x + \frac{x^{2}}{2!} +  \frac{x^{3}}{3!} + \dots +  \frac{x^{n}}{n!} + R_{n}
\end{aligned}
\end{equation}

这两个都是在$0$点展开，使用的是“Maclaurin公式”。

在实践中，往往需要做一定程度的变换。如果自己实现一种新的语言，类似于C语言或Java语言，我们可以实现一些数据类库的功能，例如计算$\sin x$或$e^{x}$，按道理上来说，是可以用“Maclaurin公式”来计算的；但是，有一个问题，“Maclaurin公式”展开之后可以有无穷多项（$n$可以取很大的值），而我们往往取前10项，它的精度就足够了。这里的描述可能有点问题。

\subsubsection{Taylor公式的应用1：计算$e^{x}$}

第01课 数学分析与概率论 01:13:40 Taylor公式的应用1：计算$e^{x}$

给定正实数$x$，计算$e^{x}=?$

一种可行的思路：

求整数$k$和小数$r$，使得$x=k \cdot \ln 2 + r$，其中$|r| \leq 0.5 \cdot \ln 2$

从而

\begin{equation}
\begin{aligned}
e^{x} &= e^{k \cdot \ln 2 + r} \\
&=e^{k \cdot \ln 2} \cdot e^{r} \\
&=2^{k} \cdot e^{r}
\end{aligned}
\end{equation}

\subsubsection{Taylor公式的应用2}

第01课 数学分析与概率论 01:14:53 Taylor公式的应用2

考察Gini系数的图像、熵、分类误差率三者之间的关系：将$f(x)=-\ln x$在$x=1$处一阶展开，忽略高阶无穷小，得到$f(x) \approx 1-x$。

上述结论，在决策树章节会进一步讨论。

\subsection{方向导数}

第01课 数学分析与概率论 01:18:49 方向导数

上面的部分讲的是“一元的导数”，如果是“二元”或“多元”的呢？

如果函数$z=f(x,y)$在点$P(x,y)$是可微分的，那么函数在该点任一方向$L$的方向导数都存在，且有：

\begin{equation}
\begin{aligned}
\frac{\partial f}{\partial l}
&= \left ( \frac{\partial f}{\partial x},\frac{\partial f}{\partial y}  \right ) \cdot \binom{\cos \varphi}{\sin \varphi} \\
&=\frac{\partial f}{\partial x} \cos \varphi + \frac{\partial f}{\partial y} \sin \varphi
\end{aligned}
\end{equation}

其中，$\varphi$为$x$轴到方向$L$的转角，$\left ( \frac{\partial f}{\partial x},\frac{\partial f}{\partial y}  \right )$是和“方向”\textbf{无关}的，而$\binom{\cos \varphi}{\sin \varphi}$是和“方向”\textbf{有关}的。


我的问题：“可微分”的概念怎么理解呢？

\subsubsection{梯度}

第01课 数学分析与概率论 01:20:56 梯度

设函数$z=f(x,y)$在平面区域$D$内具有一阶连续偏导数，则对于每一个点$P(x,y) \in D$，向量

\begin{equation}
\begin{aligned}
\nabla_{(x,y)} = \left ( \frac{\partial f}{\partial x},\frac{\partial f}{\partial y}  \right )
\end{aligned}
\end{equation}

为函数$z=f(x,y)$在点$P$的梯度，记做$gradf(x,y)$

梯度的方向是函数在该点变化最快的方向：考虑一座山，假如它的解析式为$z=H(x,y)$，在$(x_{0},y_{0})$的梯度是在该点坡度变化最快的方向。

梯度下降法。思考：若下山方向和梯度方向呈$\theta$角，下降速度是多少？

“导数”这个概念，把它从“一元”扩展到“$n$元”就得到了“梯度”。“梯度”这个概念，把它从“$n$元”降到“一元”就是“导数”。在后续的讨论中，这两个词（“导数”和“梯度”）往往是不做严格区分的。

\subsection{概率论}

第01课 数学分析与概率论 01:23:18 概率论

对概率的认识：$P(x) \in [0,1]$：（1）若$P=0$，则事件出现的概率为0，但并不表示事件一定不会发生（一根针，扎到一个平面上，它的每一点的概率都为0，但是它还是会发生的）。（2）若$x$为“离散”或“连续”变量，则$P(x=x_{0})$表示$x_{0}$发生的“概率”或“概率密度”。

累计分布函数$\Phi (x) = P(0 \leq x_{0})$：（1）$\Phi (x)$一定为单增函数；（2）$\min (\Phi (x))=0$，$\max (\Phi (x))=1$

思考：将值域为$[0,1]$的某单增函数$y=F(x)$看成$X$事件的累积概率函数：若$y=F(x)$可导，则$f(x)=F'(x)$为某概率密度函数。

cumulative distribution function, CDF 累积分布函数

Probability Density Function, PDF 概率密度函数

\subsubsection{古典概型}

第01课 数学分析与概率论 01:33:36 古典概型

“古曲概型”，我的理解是“古典的概率模型”

举例：将$n$个不同的球放入$N$（$N \geq n$）个盒子中，假设盒子容量无限，求事件$A$=\{每个盒子至多有1个球\}的概率。

只要是“古典概型”，要解决它，直接三步走：第一步，算一下所有的“基本事件总数”；第二步，算一下有效事件的总数；第三显，二者相除就可以了。

解：$P(A)=\frac{P_{N}^{n}}{N^{n}}$

基本事件总数：第1个球，有$N$种放法；第2个球，有$N$种放法；……共$N^{n}$种放法。

每个盒子至多放1个球的事件数：第1个球，有$N$种放法；第2个球，有$N-1$种放法；第3个球，有有$N-2$种放法；……共$N(N-1)(N-2)\dots(N-n+1)=P_{N}^{n}$

\subsubsection{生日悖论}

第01课 数学分析与概率论 01:36:41 生日悖论

假定班里有50位同学，则至少有2个生日相同的概率是多少？

答案是：$1-\frac{P_{N}^{n}}{N^{n}}=1-\frac{P_{365}^{50}}{365^{50}}$

\subsubsection{装箱问题}

第01课 数学分析与概率论 01:38:55 装箱问题

将12件正品和3件次品随机装在3个箱子中，每箱装5件，则每箱中恰好有1件次品的概率是多少？

我现在的问题是：无法解决每箱放5件的条件。

解：

将15件产品装入3个箱子，每箱装5件，共有$\frac{15!}{5!\cdot5!\cdot5!}$种装法

先把3件次品放入3个箱子，有$3!$种装法。对于这样的每一种装法，把其余12件产品装入3个箱子，每箱装4件，共有$\frac{12!}{4!\cdot4!\cdot4!}$种装法

\begin{equation}
\begin{aligned}
P(A)=\frac{\frac{3!\cdot12!}{4!\cdot4!\cdot4!}}{\frac{15!}{5!\cdot5!\cdot5!}}=\frac{25}{91}
\end{aligned}
\end{equation}

\subsubsection{与组合数的关系}

第01课 数学分析与概率论 01:41:23 与组合数的关系

把$n$个物品分成$k$组，使得每组物品的个数分别为$n_{1},n_{2},\dots,n_{k}$，($n=n_{1}+n_{2}+\dots+n_{k}$)，则不同的分组方法有$\frac{n!}{n_{1}!n_{2}!n_{3}!\dots n_{k}!}$种。

上述问题的简化版本，即$n$个物品分成2组，第一组$m$个，第二组$n-m$个，则分组方法有$\frac{n!}{m!(n-m)!}$，即$C_{n}^{m}$。

\subsubsection{组合数背后的秘密}

第01课 数学分析与概率论 01:42:24 组合数背后的秘密

$N \rightarrow \infty \Rightarrow \ln N! \rightarrow N(\ln N -1)$

\begin{equation}
\begin{aligned}
H&=\frac{1}{N} \ln \frac{N!}{\prod_{i=1}^{k}n_{i}!}=\frac{1}{N} \ln (N!) - \frac{1}{N} \sum_{i=1}^{k} \ln (n_{i}!) \\
&=(\ln N -1)-\frac{1}{N} \sum_{i=1}^{k} n_{i}(\ln n_{i}-1) \\
&=\ln N-\frac{1}{N} \sum_{i=1}^{k} n_{i}\ln n_{i}
=-\frac{1}{N} \left (  \left (  \sum_{i=1}^{k} n_{i}\ln n_{i}  \right ) - N\ln N\right ) \\
&=-\frac{1}{N} \sum_{i=1}^{k} \left (  n_{i}\ln n_{i} - n_{i}\ln N  \right )
=-\frac{1}{N} \sum_{i=1}^{k} \left ( n_{i}\ln \frac{n_{i}}{N} \right ) \\
&=-\sum_{i=1}^{k} \left ( \frac{n_{i}}{N}  \ln \frac{n_{i}}{N} \right )
\rightarrow -\sum_{i=1}^{k} \left ( p_{i} \ln p_{i}  \right )
\end{aligned}
\end{equation}

这个$H$叫作“熵”。

\subsubsection{商品推荐}

第01课 数学分析与概率论 01:45:46 商品推荐

商品推荐场景中过于聚焦的商品推荐往往会损害用户的购物体验，在有些场景中，系统会通过一定程度的随机性给用户带来发现的惊喜感。

假设某推荐场景中，经计算A和B两个商品与当前访问用户的匹配度分别为0.8分和0.2分，系统将随机为A商品生成一个均匀分布于0到0.8的最终得分，为B商品生成一个均匀分布于0到0.2的最终得分，试计算最终B的分数大于A的分数的概率。

这里没有听明白。这里的概念大概是“几何概型”。

\textbf{古典概型}的基本事件都是有限的，概率为事件所包含的基本事件除以总基本事件个数。
\textbf{几何概型}的基本事件通常不可计数，只能通过一定的测度，像长度，面积，体积的的比值来表示


\subsection{概率公式}

第01课 数学分析与概率论 01:49:46 概率公式

条件概率：

\begin{equation}
\begin{aligned}
P(A|B)=\frac{P(AB)}{P(B)}
\end{aligned}
\end{equation}

全概率公式：

\begin{equation}
\begin{aligned}
P(A)=\sum_{i}P(AB_{i})=\sum_{i}P(A|B_{i})P(B_{i})
\end{aligned}
\end{equation}

贝叶斯（Bayes）公式：

\begin{equation}
\begin{aligned}
P(B_{i}|A)=\frac{P(AB_{i})}{P(A)}=\frac{P(A|B_{i})P(B_{i})}{\sum_{j}P(A|B_{j})P(B_{j})}
\end{aligned}
\end{equation}

思考题：8支步枪中有5支校准过，3支未校准。一名射手用校准过的枪射击，中靶概率为0.8；用未校准的枪射击，中靶概率为0.3；现从8支枪中随机取1支，结果中靶。求该枪是已校准过的概率。

\subsubsection{两种认识下的两个学派}

第01课 数学分析与概率论 01:53:35 两种认识下的两个学派


\section{数学知识}
    1.LATEX控制序列的概念（类似于函数） \\ 控制序列可以是作为命令：以“$\backslash$”开头，参数：必须参数和可选参数。
\subsubsection{Probability Density Functions}

Probability Density Functions https://onlinecourses.science.psu.edu/stat414/node/97

\section{Vocabulary}

过拟合

样本

重采样

标记、打标签

逻辑回归、SVM、随机森林、决策树

generalized linear model (GLM)

\section{论算法之间的区别}

各自想法的出处

自己的总结

\section{面试}

\subsection{监督分类和非监督分类都有哪些呢？}

\subsection{监督分类}

线性回归

逻辑回归


\subsection{非监督分类}

聚类：K-means

PCA

LDA（文档主题模型）

\section{数学家的故事}

\subsection{欧拉}

欧拉（L.Euler,1707.4.15-1783.9.18）是瑞士数学家。生于瑞士的巴塞尔（Basel），卒于彼得堡（Petepbypt）。父亲保罗·欧拉是位牧师，喜欢数学，所以欧拉从小就受到这方面的熏陶。但父亲却执意让他攻读神学，以便将来接他的班。幸运的是，欧拉并没有走父亲为他安排的路。父亲曾在巴塞尔大学上过学，与当时著名数学家约翰·伯努利（Johann Bernoulli,1667.8.6-1748.1.1）及雅各布·伯努利（Jacob Bernoulli,1654.12.27-1705.8.16）有几分情谊。由于这种关系，欧拉结识了约翰的两个儿子：擅长数学的尼古拉（Nicolaus Bernoulli,1695-1726）及丹尼尔（Daniel Bernoulli,1700.2.9-1782.3.17）兄弟二人，（这二人后来都成为数学家）。他俩经常给小欧拉讲生动的数学故事和有趣的数学知识。这些都使欧拉受益匪浅。1720年，由约翰保举，才13岁的欧拉成了巴塞尔大学的学生，而且约翰精心培育着聪明伶俐的欧拉。当约翰发现课堂上的知识已满足不了欧拉的求知欲望时，就决定每周六下午单独给他辅导、答题和授课。约翰的心血没有白费，在他的严格训练下，欧拉终于成长起来。他17岁的时候，成为巴塞尔有史以来的第一个年轻的硕士，并成为约翰的助手。在约翰的指导下，欧拉从一开始就选择通过解决实际问题进行数学研究的道路。1726年，19岁的欧拉由于撰写了《论桅杆配置的船舶问题》而荣获巴黎科学院的资金。这标志着欧拉的羽毛已丰满，从此可以展翅飞翔。

欧拉的成长与他这段历史是分不开的。当然，欧拉的成才还有另一个重要的因素，就是他那惊人的记忆力！，他能背诵前一百个质数的前十次幂，能背诵罗马诗人维吉尔（Virgil）的史诗Aeneil，能背诵全部的数学公式。直至晚年，他还能复述年轻时的笔记的全部内容。高等数学的计算他可以用心算来完成。

尽管他的天赋很高，但如果没有约翰的教育，结果也很难想象。由于约翰·伯努利以其丰富的阅历和对数学发展状况的深刻的了解，能给欧拉以重要的指点，使欧拉一开始就学习那些虽然难学却十分必要的书，少走了不少弯路。这段历史对欧拉的影响极大，以至于欧拉成为大科学家之后仍不忘记育新人，这主要体现在编写教科书和直接培养有才华的数学工作者，其中包括后来成为大数学家的拉格朗日（J.L.Lagrange,1736.1.25-1813.4.10）。

欧拉本人虽不是教师，但他对教学的影响超过任何人。他身为世界上第一流的学者、教授，肩负着解决高深课题的重担，但却能无视"名流"的非议，热心于数学的普及工作。他编写的《无穷小分析引论》、《微分法》和《积分法》产生了深远的影响。有的学者认为，自从1784年以后，初等微积分和高等微积分教科书基本上都抄袭欧拉的书，或者抄袭那些抄袭欧拉的书。欧拉在这方面与其它数学家如高斯（C.F.Gauss,1777.4.30-1855.2.23）、牛顿（I.Newton,1643.1.4-1727.3.31）等都不同，他们所写的书一是数量少，二是艰涩难明，别人很难读懂。而欧拉的文字既轻松易懂，堪称这方面的典范。他从来不压缩字句，总是津津有味地把他那丰富的思想和广泛的兴趣写得有声有色。他用德、俄、英文发表过大量的通俗文章，还编写过大量中小学教科书。他编写的初等代数和算术的教科书考虑细致，叙述有条有理。他用许多新的思想的叙述方法，使得这些书既严密又易于理解。欧拉最先把对数定义为乘方的逆运算，并且最先发现了对数是无穷多值的。他证明了任一非零实数Ｒ有无穷多个对数。欧拉使三角学成为一门系统的科学，他首先用比值来给出三角函数的定义，而在他以前是一直以线段的长作为定义的。欧拉的定义使三角学跳出只研究三角表这个圈子。欧拉对整个三角学作了分析性的研究。在这以前，每个公式仅从图中推出，大部分以叙述表达。欧拉却从最初几个公式解析地推导出了全部三角公式，还获得了许多新的公式。欧拉用a 、b 、c 表示三角形的三条边，用Ａ、Ｂ、Ｃ表示第个边所对的角，从而使叙述大大地简化。欧拉得到的著名的公式：

又把三角函数与指数函联结起来。

在普及教育和科研中，欧拉意识到符号的简化和规则化既有有助于学生的学习，又有助于数学的发展，所以欧拉创立了许多新的符号。如用sin 、cos 等表示三角函数，用 e 表示自然对数的底，用f(x) 表示函数，用 ∑表示求和，用 i表示虚数等。圆周率π虽然不是欧拉首创，但却是经过欧拉的倡导才得以广泛流行。而且，欧拉还把e 、π 、i 统一在一个令人叫绝的关系式 中。 欧拉在研究级数时引入欧拉常数Ｃ， 这是继π 、e 之后的又一个重要的数。

欧拉不但重视教育，而且重视人才。当时法国的拉格朗日只有19岁，而欧拉已48岁。拉格朗日与欧拉通信讨论"等周问题"，欧拉也在研究这个问题。后来拉格朗日获得成果，欧拉就压下自己的论文，让拉格朗日首先发表，使他一举成名。

欧拉19岁大学毕业时，在瑞士没有找到合适的工作。1727年春，在巴塞尔他试图担任空缺的教研室主任职务，但没有成功。这时候，俄国的圣彼得堡科院刚建立不久，正在全国各地招聘科学家，广泛地搜罗人才。已经应聘在彼得堡工作的丹尔·伯努利深知欧拉的才能，因此，他竭力聘请欧拉去俄罗斯。在这种情况下，欧拉离开了自己的祖国。由于丹尼尔的推荐，1727年，欧拉应邀到圣彼得堡做丹尼尔的助手。在圣彼得堡科学院，他顺利地获得了高等数学副教授的职位。1731年，又被委任领导理论物理和实验物理教研室的工作。1733年，年仅26岁的欧拉接替回瑞士的丹尼尔，成为数学教授及彼得堡科学院数学部的领导人。

在这期间，欧拉勤奋地工作，发表了大量优秀的数学论文，以及其它方面的论文、著作。

古典力学的基础是牛顿奠定的，而欧拉则是其主要建筑师。1736年，欧拉出版了《力学，或解析地叙述运动的理论》，在这里他最早明确地提出质点或粒子的概念，最早研究质点沿任意一曲线运动时的速度，并在有关速度与加速度问题上应用矢量的概念。

同时，他创立了分析力学、刚体力学，研究和发展了弹性理论、振动理论以及材料力学。并且他把振动理论应用到音乐的理论中去，1739年，出版了一部音乐理论的著作。1738年，法国科学院设立了回答热本质问题征文的奖金，欧拉的《论火》一文获奖。在这篇文章中，欧拉把热本质看成是分子的振动。

欧拉研究问题最鲜明的特点是：他把数学研究之手深入到自然与社会的深层。他不仅是位杰出的数学家，而且也是位理论联系实际的巨匠，应用数学大师。他喜欢搞特定的具体问题，而不象现代某些数学家那样，热衰于搞一般理论。

\section{其它知识}

\subsection{数据处理——One-Hot Encoding}

原文地址： http://blog.csdn.net/google19890102/article/details/44039761

在实际的机器学习的应用任务中，\textbf{特征}有时候并不总是连续值，有可能是一些分类值，如性别可分为“male”和“female”。在机器学习任务中，对于这样的特征，通常我们需要对其进行特征数字化，如下面的例子：
有如下三个特征属性：

性别：["male"，"female"]

地区：["Europe"，"US"，"Asia"]

浏览器：["Firefox"，"Chrome"，"Safari"，"Internet Explorer"]

\textbf{不合适的方法}。对于某一个样本，如["male"，"US"，"Internet Explorer"]，我们需要将这个分类值的特征数字化，最直接的方法，我们可以采用序列化的方式：[0,1,3]。但是这样的特征处理并不能直接放入机器学习算法中。

\textbf{合适的方法}。对于上述的问题，性别的属性是二维的，同理，地区是三维的，浏览器则是四维的，这样，我们可以采用One-Hot编码的方式对上述的样本“["male"，"US"，"Internet Explorer"]”编码，“male”则对应着[1，0]，同理“US”对应着[0，1，0]，“Internet Explorer”对应着[0,0,0,1]。则完整的特征数字化的结果为：[1,0,0,1,0,0,0,0,1]。这样导致的一个结果就是数据会变得非常的稀疏。

实际的Python代码如下：

from sklearn import preprocessing

enc = preprocessing.OneHotEncoder()

enc.fit([[0,0,3],[1,1,0],[0,2,1],[1,0,2]])

array = enc.transform([[0,1,3]]).toarray()

print array

结果：[[ 1.  0.  0.  1.  0.  0.  0.  0.  1.]]

\subsection{什么是张量 (tensor)？}
什么是张量 (tensor)？ https://www.zhihu.com/question/20695804

张量的数学与物理意义是什么，张量的特性与优势是什么？ https://www.zhihu.com/question/36814916

怎么通俗地理解张量? https://www.zhihu.com/question/23720923

What is a tensor? https://www.quora.com/What-is-a-tensor


小学课本上画杨桃的故事每个人都听过，一个杨桃在不同角度看，就会呈现不同的样子。有些物理量也是一样的，它在不同的角度看就会有不同的数值。比如对于一个矢量，你的基底变化了，矢量的表示也会变化。但是矢量的长度永远不变。杨桃还是那个杨桃，物理量也还是那个物理量，但是一旦你换了个角度看，杨桃的形状就变了，物理量的数值也就变了。那么如果一个物理系统没有一个更好的观察方向，或者说我们需要频繁的变换我们的视角的时候，应该怎么把握一个胡乱变化的东西呢？你要记住，杨桃和物理量本身都是不变的，变的只是它在你眼中的形象。于是张量就出现了，它将视角变换时候的变换关系作为张量的定义，看似在乱七八糟变，实际上只有满足这样的变换关系，它才是不变的！研究一个看似乱七八糟变，实际上不变的东西，就是张量分析。

作者：大野喵渣
链接：https://www.zhihu.com/question/36814916/answer/69248640
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

\subsection{数学符号“s.t.”的意义}

在优化问题的求解中，如线性规划、非线性规划问题等，经常会遇到数学符号“s.t.”，它的意思是什么呢？

“s.t.”，指 subject to，受限制于...。

例如：

                目标函数：min {x+2}

                约束条件：s.t.  x={1,2,3}

其题意为，求x+2的最小值以使得x的取值为1、2、3时。

或者理解为，x的取值为1、2、3时，求x+2的最小值。


\end{document}

