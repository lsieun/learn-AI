# Linear Regression #

## 1、概念 ##

### 什么是“回归分析（regression analysis)”？ ###

回归分析（regression analysis)是确定/两种或两种以上/变量之间相互依赖的/定量关系的/一种统计分析方法。

回归分析的分类：

- 按照涉及的变量的多少，分为一元回归和多元回归分析
- 按照因变量的多少，可分为简单回归分析和多重回归分析
- 按照自变量和因变量之间的关系类型，可分为线性回归分析和非线性回归分析


如果在回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为【一元线性回归分析】。
如果回归分析中包括两个或两个以上的自变量，且自变量之间存在线性相关，则称为【多重线性回归分析】。

在**大数据分析**中，**回归分析**是一种**预测性**的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。这种技术通常用于**预测分析**，时间序列模型以及发现变量之间的因果关系。

【lsieun】回归分析(regression analysis)，让我想到了“道生一，一生二，二生三，三生万物”，说明“万物是由"简单"的**道**慢慢发展而来的，那么如果掌握了**道**，就相当于有了推演/预测万物的本事了”；单词regression，其中gress的词根表示“走”，progress=pre+gress，表示“向前走、进步”，而regress=re+gress，表示“倒退、退化、回归”，那regression anaylysis表示“从一堆数据当中，是寻找产生这一堆数据的规律”，有了这个规律，我们就可以对将来的数据进行预测了。

### 什么是“线性回归(Linear Regression)”？ ###

线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为y = w'x+e，e为误差服从均值为0的正态分布。

拟合方程：最小二乘法

	一般来说，线性回归都可以通过最小二乘法求出其方程，可以计算出对于y=bx+a的直线。




参考文章：[对线性回归、逻辑回归、各种回归的概念学习](http://blog.csdn.net/viewcode/article/details/8794401)

# 对线性回归、逻辑回归、各种回归的概念学习 #

回归问题的条件/前提：
1） 收集的数据
2） 假设的模型，即一个函数，这个函数里含有未知的参数，通过学习，可以估计出参数。然后利用这个模型去预测/分类新的数据。

## 线性回归 ##

假设 **特征** 和 **结果** 都满足线性。即不大于一次方。这个是针对 收集的数据而言。
收集的数据中，每一个分量，就可以看做一个特征数据。每个特征至少对应一个未知的参数。这样就形成了一个线性模型函数，向量表示形式：


