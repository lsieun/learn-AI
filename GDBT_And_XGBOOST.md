# GBDT 和 XGBOOST #

最初的GBDT发展到现在的XGBoost，改进是一点一滴来的，是一篇篇论文的积累，很多方法并非XGBoost第一次提出，当然也不是说XGBoost没改进，可以说XGBoost把算法和系统实现都做得淋漓尽致。

## Boosting ##

Boosting方法是一种用来提高【弱分类算法】准确度的方法,这种方法通过构造【一系列预测函数】,然后以一定的方式将他们组合成【一个预测函数】。

### Boosting算法起源 ###

【Boosting】是一种【提高任意给定学习算法准确度】的方法。它的思想起源于 Valiant提出的 【PAC ( Probably Approximately Correct)学习模型】。

Valiant和 Kearns提出了【弱学习】和【强学习】的概念 ,识别错误率小于1/2,也即**准确率仅比随机猜测略高**的学习算法称为【弱学习算法】;**识别准确率很高**并能在**多项式时间内**完成的学习算法称为【强学习算法】。同时 ,Valiant和 Kearns首次提出了 【PAC学习模型中弱学习算法】和【强学习算法】的等价性问题,即**任意给定仅比随机猜测略好的弱学习算法 ,是否可以将其提升为强学习算法** ? 如果二者等价 ,那么**只需找到一个比随机猜测略好的弱学习算法就可以将其提升为强学习算法 ,而不必寻找很难获得的强学习算法**。

1990年, Schapire最先构造出一种【多项式级的算法】 ,**对该问题做了肯定的证明** ,这就是**最初的 Boosting算法**。一年后 ,Freund提出了一种效率更高的Boosting算法。**但是,这两种算法存在共同的实践上的缺陷,那就是都要求事先知道【弱学习算法学习正确的下限】**。

1995年 , Freund和 schap ire改进了Boosting算法 ,提出了 AdaBoost (Adap tive Boosting)算法,该算法效率和 Freund于 1991年提出的 Boosting算法几乎相同 ,**但不需要任何关于弱学习器的先验知识 ,因而更容易应用到实际问题当中**。之后 , Freund和 schapire进一步提出了改变 Boosting投票权重的 AdaBoost . M1,AdaBoost . M2等算法 ,在机器学习领域受到了极大的关注。


### Boosting方法概述 ###

Boosting方法是一种用来提高弱分类算法准确度的方法,这种方法通过构造【一个预测函数系列】,然后以一定的方式将他们组合成【一个预测函数】。他是一种【框架算法】,主要是通过对【样本集】的操作获得【样本子集】,然后用【弱分类算法】在【样本子集】上训练生成【一系列的基分类器】。他可以用来提高其他【弱分类算法】的识别率,也就是将【其他的弱分类算法】作为【基分类算法】放于Boosting 框架中,通过Boosting框架对训练样本集的操作,得到不同的【训练样本子集】,用【该样本子集】去训练生成【基分类器】;每得到【一个样本集】就用【该基分类算法】在该样本集上产生【一个基分类器】,这样在给定训练轮数 n 后,就可产生 n 个基分类器,然后【Boosting框架算法】将这 【n个基分类器】进行加权融合,产生一个最后的结果分类器,在这 【n个基分类器】中,每个【单个的分类器】的识别率不一定很高,但他们联合后的结果有很高的识别率,这样便提高了该弱分类算法的识别率。在产生单个的基分类器时可用相同的分类算法,也可用不同的分类算法,这些算法一般是不稳定的弱分类算法,如神经网络(BP) ,决策树(C4.5)等。













































